{"version":3,"file":"urql-exchange-graphcache.min.js","sources":["../src/ast/node.ts","../src/ast/variables.ts","../src/helpers/help.ts","../src/ast/traversal.ts","../src/ast/schema.ts","../src/ast/schemaPredicates.ts","../src/store/keys.ts","../src/helpers/dict.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/operations/invalidate.ts","../src/store/store.ts","../src/operations/query.ts","../src/helpers/operation.ts","../src/cacheExchange.ts","../src/offlineExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : node.name.value;\n\nconst emptySelectionSet: SelectionSet = [];\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet ? node.selectionSet.selections : emptySelectionSet;\n\nexport const getTypeCondition = (node: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  node.typeCondition ? node.typeCondition.name.value : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  let args: null | Variables = null;\n  if (node.arguments) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        if (!args) args = {};\n        args[getName(arg)] = value as any;\n      }\n    }\n  }\n  return args;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | Record<string, unknown>\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 14\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25\n  | 26;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import {\n  SelectionNode,\n  DocumentNode,\n  OperationDefinitionNode,\n  FragmentSpreadNode,\n  InlineFragmentNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  for (let i = 0; i < doc.definitions.length; i++) {\n    if (doc.definitions[i].kind === Kind.OPERATION_DEFINITION) {\n      return doc.definitions[i] as OperationDefinitionNode;\n    }\n  }\n\n  invariant(\n    false,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments => {\n  const fragments: Fragments = {};\n  for (let i = 0; i < doc.definitions.length; i++) {\n    const node = doc.definitions[i];\n    if (node.kind === Kind.FRAGMENT_DEFINITION) {\n      fragments[getName(node)] = node;\n    }\n  }\n\n  return fragments;\n};\n\n/** Resolves @include and @skip directives to determine whether field is included. */\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n\n/** Resolves @defer directive to determine whether a fragment is potentially skipped. */\nexport const isDeferred = (\n  node: FragmentSpreadNode | InlineFragmentNode,\n  vars: Variables\n): boolean => {\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (name === 'defer') {\n      for (\n        let j = 0;\n        directive.arguments && j < directive.arguments.length;\n        j++\n      ) {\n        const argument = directive.arguments[i];\n        if (getName(argument) === 'if') {\n          // Return whether `@defer(if: )` is enabled\n          return !!valueFromASTUntyped(argument.value, vars);\n        }\n      }\n\n      return true;\n    }\n  }\n\n  return false;\n};\n","import {\n  IntrospectionQuery,\n  IntrospectionSchema,\n  IntrospectionInputValue,\n  IntrospectionTypeRef,\n  IntrospectionType,\n} from 'graphql';\n\nexport interface SchemaField {\n  name: string;\n  type: IntrospectionTypeRef;\n  args(): Record<string, IntrospectionInputValue | void>;\n}\n\nexport interface SchemaObject {\n  name: string;\n  kind: 'INTERFACE' | 'OBJECT';\n  interfaces(): Record<string, unknown>;\n  fields(): Record<string, SchemaField | void>;\n}\n\nexport interface SchemaUnion {\n  name: string;\n  kind: 'UNION';\n  types(): Record<string, unknown>;\n}\n\nexport interface SchemaIntrospector {\n  query: string | null;\n  mutation: string | null;\n  subscription: string | null;\n  types?: Map<string, SchemaObject | SchemaUnion>;\n  isSubType(abstract: string, possible: string): boolean;\n}\n\nexport interface PartialIntrospectionSchema {\n  queryType: { name: string; kind?: any };\n  mutationType?: { name: string; kind?: any } | null;\n  subscriptionType?: { name: string; kind?: any } | null;\n  types?: IntrospectionSchema['types'];\n}\n\nexport type IntrospectionData =\n  | IntrospectionQuery\n  | { __schema: PartialIntrospectionSchema };\n\nexport const buildClientSchema = ({\n  __schema,\n}: IntrospectionData): SchemaIntrospector => {\n  const typemap: Map<string, SchemaObject | SchemaUnion> = new Map();\n\n  const buildNameMap = <T extends { name: string }>(\n    arr: ReadonlyArray<T>\n  ): (() => { [name: string]: T }) => {\n    let map: Record<string, T> | void;\n    return () => {\n      if (!map) {\n        map = {};\n        for (let i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];\n      }\n      return map;\n    };\n  };\n\n  const buildType = (\n    type: IntrospectionType\n  ): SchemaObject | SchemaUnion | void => {\n    switch (type.kind) {\n      case 'OBJECT':\n      case 'INTERFACE':\n        return {\n          name: type.name,\n          kind: type.kind as 'OBJECT' | 'INTERFACE',\n          interfaces: buildNameMap(type.interfaces || []),\n          fields: buildNameMap(\n            type.fields.map(field => ({\n              name: field.name,\n              type: field.type,\n              args: buildNameMap(field.args),\n            }))\n          ),\n        } as SchemaObject;\n      case 'UNION':\n        return {\n          name: type.name,\n          kind: type.kind as 'UNION',\n          types: buildNameMap(type.possibleTypes || []),\n        } as SchemaUnion;\n    }\n  };\n\n  const schema: SchemaIntrospector = {\n    query: __schema.queryType ? __schema.queryType.name : null,\n    mutation: __schema.mutationType ? __schema.mutationType.name : null,\n    subscription: __schema.subscriptionType\n      ? __schema.subscriptionType.name\n      : null,\n    types: undefined,\n    isSubType(abstract: string, possible: string) {\n      const abstractType = typemap.get(abstract);\n      const possibleType = typemap.get(possible);\n      if (!abstractType || !possibleType) {\n        return false;\n      } else if (abstractType.kind === 'UNION') {\n        return !!abstractType.types()[possible];\n      } else if (\n        abstractType.kind !== 'OBJECT' &&\n        possibleType.kind === 'OBJECT'\n      ) {\n        return !!possibleType.interfaces()[abstract];\n      } else {\n        return abstract === possible;\n      }\n    },\n  };\n\n  if (__schema.types) {\n    schema.types = typemap;\n    for (let i = 0; i < __schema.types.length; i++) {\n      const type = __schema.types[i];\n      if (type && type.name) {\n        const out = buildType(type);\n        if (out) typemap.set(type.name, out);\n      }\n    }\n  }\n\n  return schema;\n};\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport { getTypeCondition } from './node';\nimport { SchemaIntrospector, SchemaObject } from './schema';\n\nimport {\n  KeyingConfig,\n  UpdateResolver,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_NAME = '__';\n\nexport const isFieldNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  return !!field && field.type.kind !== 'NON_NULL';\n};\n\nexport const isListNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType =\n    field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;\n  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';\n};\n\nexport const isFieldAvailableOnType = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean =>\n  fieldName.indexOf(BUILTIN_NAME) === 0 ||\n  typename.indexOf(BUILTIN_NAME) === 0 ||\n  !!getField(schema, typename, fieldName);\n\nexport const isInterfaceOfType = (\n  schema: SchemaIntrospector,\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: string | void\n): boolean => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) {\n    return true;\n  } else if (\n    schema.types!.has(typeCondition) &&\n    schema.types!.get(typeCondition)!.kind === 'OBJECT'\n  ) {\n    return typeCondition === typename;\n  }\n\n  expectAbstractType(schema, typeCondition!);\n  expectObjectType(schema, typename!);\n  return schema.isSubType(typeCondition, typename);\n};\n\nconst getField = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n) => {\n  if (\n    fieldName.indexOf(BUILTIN_NAME) === 0 ||\n    typename.indexOf(BUILTIN_NAME) === 0\n  )\n    return;\n\n  expectObjectType(schema, typename);\n  const object = schema.types!.get(typename) as SchemaObject;\n  const field = object.fields()[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types!.has(typename) &&\n      schema.types!.get(typename)!.kind === 'OBJECT',\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types!.has(typename) &&\n      (schema.types!.get(typename)!.kind === 'INTERFACE' ||\n        schema.types!.get(typename)!.kind === 'UNION'),\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: SchemaIntrospector,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    for (const key in keys) {\n      if (!schema.types!.has(key)) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    }\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: SchemaIntrospector,\n  updates: Record<string, Record<string, UpdateResolver | undefined>>\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const mutationFields = (schema.types!.get(\n      schema.mutation\n    ) as SchemaObject).fields();\n    const givenMutations = updates[schema.mutation] || {};\n    for (const fieldName in givenMutations) {\n      if (mutationFields[fieldName] === undefined) {\n        warn(\n          'Invalid mutation field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n          21\n        );\n      }\n    }\n  }\n\n  if (schema.subscription) {\n    const subscriptionFields = (schema.types!.get(\n      schema.subscription\n    ) as SchemaObject).fields();\n    const givenSubscription = updates[schema.subscription] || {};\n    for (const fieldName in givenSubscription) {\n      if (subscriptionFields[fieldName] === undefined) {\n        warn(\n          'Invalid subscription field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n          22\n        );\n      }\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nfunction warnAboutAbstractResolver(\n  name: string,\n  kind: 'UNION' | 'INTERFACE'\n): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` does not match to a concrete type in the schema, but the \\`resolvers\\` option is referencing it. Implement the resolver for the types that ${\n      kind === 'UNION' ? 'make up the union' : 'implement the interface'\n    } instead.`,\n    26\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: SchemaIntrospector,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      if (schema.query) {\n        const validQueries = (schema.types!.get(\n          schema.query\n        ) as SchemaObject).fields();\n        for (const resolverQuery in resolvers.Query) {\n          if (!validQueries[resolverQuery]) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (!schema.types!.has(key)) {\n        warnAboutResolver(key);\n      } else if (\n        schema.types!.get(key)!.kind === 'INTERFACE' ||\n        schema.types!.get(key)!.kind === 'UNION'\n      ) {\n        warnAboutAbstractResolver(\n          key,\n          schema.types!.get(key)!.kind as 'INTERFACE' | 'UNION'\n        );\n      } else {\n        const validTypeProperties = (schema.types!.get(\n          key\n        ) as SchemaObject).fields();\n        for (const resolverProperty in resolvers[key]) {\n          if (!validTypeProperties[resolverProperty]) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: SchemaIntrospector,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const validMutations = (schema.types!.get(\n      schema.mutation\n    ) as SchemaObject).fields();\n    for (const mutation in optimisticMutations) {\n      if (!validMutations[mutation]) {\n        warn(\n          `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n          24\n        );\n      }\n    }\n  }\n}\n","import { stringifyVariables } from '@urql/core';\nimport { FieldArgs, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: FieldArgs) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","export const makeDict = (): any => Object.create(null);\n\nexport const isDictEmpty = (x: any) => {\n  for (const _ in x) return false;\n  return true;\n};\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n  Data,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OperationMap<T> = Map<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OperationMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: KeyMap<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OperationMap<KeyMap<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and deferred/streamed */\n  deferredKeys: Set<number>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOwnership: null | WeakSet<Data> = null;\nlet currentDataMapping: null | WeakMap<Data, Data> = null;\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentOptimistic = false;\n\n/** Creates a new data object unless it's been created in this data run */\nexport const makeData = (data?: Data): Data => {\n  let newData: Data;\n  if (data) {\n    if (currentOwnership!.has(data)) return data;\n    newData = currentDataMapping!.get(data) || ({ ...data } as Data);\n    currentDataMapping!.set(data, newData);\n  } else {\n    newData = {} as Data;\n  }\n\n  currentOwnership!.add(newData);\n  return newData;\n};\n\nexport const isWriting = (): boolean => currentOperation === 'write';\n\nexport const ownsData = (data?: Data): boolean =>\n  !!data && currentOwnership!.has(data);\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey?: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOwnership = new WeakSet();\n  currentDataMapping = new WeakMap();\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = new Set();\n  currentOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (currentOperation === 'read') {\n    // We don't create new layers for read operations and instead simply\n    // apply the currently available layer, if any\n    currentOptimisticKey = layerKey;\n  } else if (isOptimistic || data.optimisticOrder.length > 1) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      if (\n        data.optimisticOrder.indexOf(layerKey) !== -1 &&\n        !data.commutativeKeys.has(layerKey)\n      ) {\n        data.optimisticOrder.splice(data.optimisticOrder.indexOf(layerKey), 1);\n      }\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutative chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    // We also do this when only one layer exists to avoid having to squash\n    // any layers at the end of writing this layer\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock.has(data.optimisticOrder[i]) &&\n      data.commutativeKeys.has(data.optimisticOrder[i]) &&\n      !data.deferredKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOwnership = null;\n  currentDataMapping = null;\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    setTimeout(() => {\n      initDataState('read', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  if (layerKey && !isOptimistic) data.deferredKeys.delete(layerKey);\n  initDataState('write', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: new Map(),\n  refLock: new Map(),\n  links: {\n    optimistic: new Map(),\n    base: new Map(),\n  },\n  records: {\n    optimistic: new Map(),\n    base: new Map(),\n  },\n  deferredKeys: new Set(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic.get(currentOptimisticKey)!\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n  // A read may be initialised to skip layers until its own, which is useful for\n  // reading back written data. It won't skip over optimistic layers however\n  let skip =\n    !currentOptimistic &&\n    currentOperation === 'read' &&\n    currentOptimisticKey &&\n    currentData!.commutativeKeys.has(currentOptimisticKey);\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic.get(layerKey);\n    // If we're reading starting from a specific layer, we skip until a match\n    skip = skip && layerKey !== currentOptimisticKey;\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!skip || !currentData!.commutativeKeys.has(layerKey)) &&\n      (!currentOptimistic ||\n        currentOperation === 'write' ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: undefined | Set<string>,\n  refCount: KeyMap<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count and adjust it by \"by\"\n  const count = refCount.get(entityKey) || 0;\n  const newCount = count + by;\n  refCount.set(entityKey, newCount);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: undefined | Set<string>,\n  refCount: KeyMap<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      if (Array.isArray(link[i])) {\n        updateRCForLink(gc, refCount, link[i], by);\n      } else if (link[i]) {\n        updateRCForEntity(gc, refCount, link[i] as string, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic.get(currentData!.optimisticOrder[i]);\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  const { gc: batch } = currentData!;\n  for (const entityKey of batch.keys()) {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount.get(entityKey) || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey of currentData!.refLock.keys()) {\n      const refCount = currentData!.refLock.get(layerKey);\n      if (refCount) {\n        const locks = refCount.get(entityKey) || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        refCount.delete(entityKey);\n      }\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    currentData!.refCount.delete(entityKey);\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  }\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: KeyMap<number> | undefined;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: undefined | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount = data.refLock.get(currentOptimisticKey);\n    if (!refCount)\n      data.refLock.set(currentOptimisticKey, (refCount = new Map()));\n    links = data.links.optimistic.get(currentOptimisticKey);\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (\n  data: InMemoryData,\n  layerKey: number,\n  hasNext?: boolean\n) => {\n  if (hasNext) {\n    data.deferredKeys.add(layerKey);\n  } else {\n    data.deferredKeys.delete(layerKey);\n  }\n\n  let index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    if (hasNext || !data.commutativeKeys.has(layerKey)) {\n      data.optimisticOrder.splice(index, 1);\n      // Protect optimistic layers from being turned into non-optimistic layers\n      // while preserving optimistic data\n      clearLayer(data, layerKey);\n    } else {\n      return;\n    }\n  }\n\n  // If the layer has future results then we'll move it past any layer that's\n  // still empty, so currently pending operations will take precedence over it\n  for (\n    index = 0;\n    hasNext &&\n    index < data.optimisticOrder.length &&\n    !data.deferredKeys.has(data.optimisticOrder[index]) &&\n    (!data.refLock.has(data.optimisticOrder[index]) ||\n      !data.commutativeKeys.has(data.optimisticOrder[index]));\n    index++\n  );\n\n  data.optimisticOrder.splice(index, 0, layerKey);\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock.has(layerKey)) {\n    data.refLock.set(layerKey, new Map());\n    data.links.optimistic.set(layerKey, new Map());\n    data.records.optimistic.set(layerKey, new Map());\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock.has(layerKey)) {\n    data.refLock.delete(layerKey);\n    data.records.optimistic.delete(layerKey);\n    data.links.optimistic.delete(layerKey);\n    data.deferredKeys.delete(layerKey);\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = new Set();\n\n  const links = currentData!.links.optimistic.get(layerKey);\n  if (links) {\n    for (const entry of links.entries()) {\n      const entityKey = entry[0];\n      const keyMap = entry[1];\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    }\n  }\n\n  const records = currentData!.records.optimistic.get(layerKey);\n  if (records) {\n    for (const entry of records.entries()) {\n      const entityKey = entry[0];\n      const keyMap = entry[1];\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    }\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentOptimistic = true;\n    currentOperation = 'read';\n    const entries: SerializedEntries = makeDict();\n    for (const key of currentData!.persist.keys()) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    }\n\n    currentOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('write', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        if (readLink(entityKey, fieldKey) === undefined)\n          writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        if (readRecord(entityKey, fieldKey) === undefined)\n          writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import { CombinedError } from '@urql/core';\nimport {\n  GraphQLError,\n  FieldNode,\n  InlineFragmentNode,\n  FragmentDefinitionNode,\n} from 'graphql';\n\nimport {\n  isDeferred,\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField, isWriting } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  DataField,\n  NullArray,\n  Link,\n  Entity,\n  Data,\n} from '../types';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  parent: Data;\n  fieldName: string;\n  error: GraphQLError | undefined;\n  partial: boolean;\n  optimistic: boolean;\n  __internal: {\n    path: Array<string | number>;\n    errorMap: { [path: string]: GraphQLError } | undefined;\n  };\n}\n\nexport const contextRef: { current: Context | null } = { current: null };\nexport const deferRef: { current: boolean } = { current: false };\n\n// Checks whether the current data field is a cache miss because of a GraphQLError\nexport const getFieldError = (ctx: Context): GraphQLError | undefined =>\n  ctx.__internal.path.length > 0 && ctx.__internal.errorMap\n    ? ctx.__internal.errorMap[ctx.__internal.path.join('.')]\n    : undefined;\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean,\n  error?: CombinedError | undefined\n): Context => {\n  const ctx: Context = {\n    store,\n    variables,\n    fragments,\n    parent: { __typename: typename },\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    error: undefined,\n    partial: false,\n    optimistic: !!optimistic,\n    __internal: {\n      path: [],\n      errorMap: undefined,\n    },\n  };\n\n  if (error && error.graphQLErrors) {\n    for (let i = 0; i < error.graphQLErrors.length; i++) {\n      const graphQLError = error.graphQLErrors[i];\n      if (graphQLError.path && graphQLError.path.length) {\n        if (!ctx.__internal.errorMap)\n          ctx.__internal.errorMap = Object.create(null);\n        ctx.__internal.errorMap![graphQLError.path.join('.')] = graphQLError;\n      }\n    }\n  }\n\n  return ctx;\n};\n\nexport const updateContext = (\n  ctx: Context,\n  data: Data,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  contextRef.current = ctx;\n  ctx.parent = data;\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n  ctx.error = getFieldError(ctx);\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return (\n    isWriting() ||\n    !getSelectionSet(node).some(node => {\n      if (!isFieldNode(node)) return false;\n      const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n      return !hasField(entityKey, fieldKey);\n    })\n  );\n};\n\ninterface SelectionIterator {\n  (): FieldNode | undefined;\n}\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n): SelectionIterator => {\n  let childDeferred = false;\n  let childIterator: SelectionIterator | void;\n  let index = 0;\n\n  return function next() {\n    if (!deferRef.current && childDeferred) deferRef.current = childDeferred;\n\n    if (childIterator) {\n      const node = childIterator();\n      if (node != null) {\n        return node;\n      }\n\n      childIterator = undefined;\n      childDeferred = false;\n      if (process.env.NODE_ENV !== 'production') {\n        popDebugNode();\n      }\n    }\n\n    while (index < select.length) {\n      const node = select[index++];\n      if (!shouldInclude(node, ctx.variables)) {\n        continue;\n      } else if (!isFieldNode(node)) {\n        // A fragment is either referred to by FragmentSpread or inline\n        const fragmentNode = !isInlineFragment(node)\n          ? ctx.fragments[getName(node)]\n          : node;\n\n        if (fragmentNode !== undefined) {\n          const isMatching = ctx.store.schema\n            ? isInterfaceOfType(ctx.store.schema, fragmentNode, typename)\n            : isFragmentHeuristicallyMatching(\n                fragmentNode,\n                typename,\n                entityKey,\n                ctx.variables\n              );\n          if (isMatching) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(typename, fragmentNode);\n            }\n\n            childDeferred = !!isDeferred(node, ctx.variables);\n            if (!deferRef.current && childDeferred)\n              deferRef.current = childDeferred;\n\n            return (childIterator = makeSelectionIterator(\n              typename,\n              entityKey,\n              getSelectionSet(fragmentNode)!,\n              ctx\n            ))();\n          }\n        }\n      } else {\n        return node;\n      }\n    }\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x == null ? null : (x as Data | NullArray<Data>);\n\nexport const ensureLink = (store: Store, ref: Link<Entity>): Link => {\n  if (ref == null) {\n    return ref;\n  } else if (Array.isArray(ref)) {\n    const link = new Array(ref.length);\n    for (let i = 0, l = link.length; i < l; i++)\n      link[i] = ensureLink(store, ref[i]);\n    return link;\n  }\n\n  const link = store.keyOfEntity(ref);\n  if (!link && ref && typeof ref === 'object') {\n    warn(\n      \"Can't generate a key for link(...) item.\" +\n        '\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        ref.__typename +\n        '`.',\n      12\n    );\n  }\n\n  return link;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n  OptimisticMutationResolver,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  if (process.env.NODE_ENV !== 'production') {\n    invariant(\n      getMainOperation(request.query).operation === 'mutation',\n      'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n        'This case is unsupported and should never occur.',\n      10\n    );\n  }\n\n  initDataState('write', store.data, key, true);\n  const result = startWrite(store, request, {} as Data, undefined, true);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  isOptimistic?: boolean\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const kind = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    kind,\n    kind,\n    !!isOptimistic,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(kind, operation);\n  }\n\n  writeSelection(ctx, kind, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables,\n  fragmentName?: string\n) => {\n  const fragments = getFragments(query);\n  let fragment: FragmentDefinitionNode;\n  if (fragmentName) {\n    fragment = fragments[fragmentName] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'writeFragment(...) was called with a fragment name that does not exist.\\n' +\n          'You provided ' +\n          fragmentName +\n          ' but could only find ' +\n          Object.keys(fragments).join(', ') +\n          '.',\n        11\n      );\n\n      return null;\n    }\n  } else {\n    const names = Object.keys(fragments);\n    fragment = fragments[names[0]] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'writeFragment(...) was called with an empty fragment.\\n' +\n          'You have to call it with at least one fragment in your GraphQL document.',\n        11\n      );\n\n      return null;\n    }\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey,\n    undefined\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    warn(\n      \"Couldn't find __typename when writing.\\n\" +\n        \"If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\",\n      14\n    );\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iterate = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[ctx.optimistic ? fieldName : fieldAlias];\n\n    // Development check of undefined fields\n    if (process.env.NODE_ENV !== 'production') {\n      if (\n        !isRoot &&\n        fieldValue === undefined &&\n        !deferRef.current &&\n        !ctx.optimistic\n      ) {\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.',\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename && fieldName !== '__typename') {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    if (\n      // Skip typename fields and assume they've already been written above\n      fieldName === '__typename' ||\n      // Fields marked as deferred that aren't defined must be skipped\n      (fieldValue === undefined && deferRef.current)\n    ) {\n      continue;\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    // Execute optimistic mutation functions on root fields, or execute recursive functions\n    // that have been returned on optimistic objects\n    let resolver: OptimisticMutationResolver | void;\n    if (ctx.optimistic && isRoot) {\n      resolver = ctx.store.optimisticMutations[fieldName];\n      if (!resolver) continue;\n    } else if (ctx.optimistic && typeof fieldValue === 'function') {\n      resolver = fieldValue as any;\n    }\n\n    // Execute the field-level resolver to retrieve its data\n    if (resolver) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, data, typename, typename, fieldKey, fieldName);\n      fieldValue = ensureData(resolver(fieldArgs || {}, ctx.store, ctx));\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        (fieldValue !== null || !getFieldError(ctx)\n          ? fieldValue\n          : undefined) as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        updateContext(\n          ctx,\n          data,\n          typename,\n          typename,\n          joinKeys(typename, fieldKey),\n          fieldName\n        );\n\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link | undefined => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      // Add the current index to the walked path before processing the link\n      ctx.__internal.path.push(i);\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, data[i], indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (data === null) {\n    return getFieldError(ctx) ? undefined : null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    !ctx.store.keys[data.__typename] &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import * as InMemoryData from '../store/data';\nimport { FieldArgs } from '../types';\nimport { keyOfField } from '../store';\n\ninterface PartialFieldInfo {\n  fieldKey: string;\n}\n\nexport const invalidateEntity = (\n  entityKey: string,\n  field?: string,\n  args?: FieldArgs\n) => {\n  const fields: PartialFieldInfo[] = field\n    ? [{ fieldKey: keyOfField(field, args) }]\n    : InMemoryData.inspectFields(entityKey);\n\n  for (let i = 0, l = fields.length; i < l; i++) {\n    const { fieldKey } = fields[i];\n    if (InMemoryData.readLink(entityKey, fieldKey) !== undefined) {\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n    } else {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    }\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { TypedDocumentNode, formatDocument, createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  FieldArgs,\n  Link,\n  Data,\n  QueryInput,\n  UpdateResolver,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  Entity,\n  CacheExchangeOpts,\n} from '../types';\n\nimport { invariant } from '../helpers/help';\nimport { contextRef, ensureLink } from '../operations/shared';\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\nimport {\n  SchemaIntrospector,\n  buildClientSchema,\n  expectValidKeyingConfig,\n  expectValidUpdatesConfig,\n  expectValidResolversConfig,\n  expectValidOptimisticMutationsConfig,\n} from '../ast';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store<\n  C extends Partial<CacheExchangeOpts> = Partial<CacheExchangeOpts>\n> implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: Record<string, Record<string, UpdateResolver | undefined>>;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: SchemaIntrospector;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: C) {\n    if (!opts) opts = {} as C;\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = buildClientSchema(opts.schema);\n      queryName = schema.query || queryName;\n      mutationName = schema.mutation || mutationName;\n      subscriptionName = schema.subscription || subscriptionName;\n      // Only add schema introspector if it has types info\n      if (schema.types) this.schema = schema;\n    }\n\n    this.updates = {\n      [mutationName]: (opts.updates && opts.updates.Mutation) || {},\n      [subscriptionName]: (opts.updates && opts.updates.Subscription) || {},\n    };\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n\n    if (this.schema && process.env.NODE_ENV !== 'production') {\n      expectValidKeyingConfig(this.schema, this.keys);\n      expectValidUpdatesConfig(this.schema, this.updates);\n      expectValidResolversConfig(this.schema, this.resolvers);\n      expectValidOptimisticMutationsConfig(\n        this.schema,\n        this.optimisticMutations\n      );\n    }\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Entity) {\n    // In resolvers and updaters we may have a specific parent\n    // object available that can be used to skip to a specific parent\n    // key directly without looking at its incomplete properties\n    if (contextRef.current && data === contextRef.current.parent)\n      return contextRef.current!.parentKey;\n\n    if (data == null || typeof data === 'string') return data || null;\n    if (!data.__typename) return null;\n    if (this.rootNames[data.__typename]) return data.__typename;\n\n    let key: string | null | void;\n    if (this.keys[data.__typename]) {\n      key = this.keys[data.__typename](data);\n    } else if (data.id != null) {\n      key = `${data.id}`;\n    } else if (data._id != null) {\n      key = `${data._id}`;\n    }\n\n    return key ? `${data.__typename}:${key}` : null;\n  }\n\n  resolve(entity: Entity, field: string, args?: FieldArgs): DataField {\n    const fieldKey = keyOfField(field, args);\n    const entityKey = this.keyOfEntity(entity);\n    if (!entityKey) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link || null;\n  }\n\n  resolveFieldByKey = this.resolve;\n\n  invalidate(entity: Entity, field?: string, args?: FieldArgs) {\n    const entityKey = this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Entity): FieldInfo[] {\n    const entityKey = this.keyOfEntity(entity);\n    return entityKey ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery<T = Data, V = Variables>(\n    input: QueryInput<T, V>,\n    updater: (data: T | null) => T | null\n  ): void {\n    const request = createRequest<T, V>(input.query, input.variables as any);\n    request.query = formatDocument(request.query);\n    const output = updater(this.readQuery(request));\n    if (output !== null) {\n      startWrite(this, request, output as any);\n    }\n  }\n\n  readQuery<T = Data, V = Variables>(input: QueryInput<T, V>): T | null {\n    const request = createRequest(input.query, input.variables!);\n    request.query = formatDocument(request.query);\n    return read(this, request).data as T | null;\n  }\n\n  readFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    entity: string | Data | T,\n    variables?: V,\n    fragmentName?: string\n  ): T | null {\n    return readFragment(\n      this,\n      formatDocument(fragment),\n      entity,\n      variables as any,\n      fragmentName\n    ) as T | null;\n  }\n\n  writeFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    data: T,\n    variables?: V,\n    fragmentName?: string\n  ): void {\n    writeFragment(\n      this,\n      formatDocument(fragment),\n      data,\n      variables as any,\n      fragmentName\n    );\n  }\n\n  link(\n    entity: Entity,\n    field: string,\n    args: FieldArgs,\n    link: Link<Entity>\n  ): void;\n\n  link(entity: Entity, field: string, link: Link<Entity>): void;\n\n  link(\n    entity: Entity,\n    field: string,\n    argsOrLink: FieldArgs | Link<Entity>,\n    maybeLink?: Link<Entity>\n  ): void {\n    const args = (maybeLink !== undefined ? argsOrLink : null) as FieldArgs;\n    const link = (maybeLink !== undefined\n      ? maybeLink\n      : argsOrLink) as Link<Entity>;\n    const entityKey = ensureLink(this, entity);\n    if (typeof entityKey === 'string') {\n      InMemoryData.writeLink(\n        entityKey,\n        keyOfField(field, args),\n        ensureLink(this, link)\n      );\n    }\n  }\n}\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n  makeData,\n  ownsData,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data | null | undefined,\n  error?: CombinedError | undefined,\n  key?: number\n): QueryResult => {\n  initDataState('read', store.data, key);\n  const result = read(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data | null | undefined,\n  error?: CombinedError | undefined\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey,\n    false,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  if (!input) input = makeData();\n  // NOTE: This may reuse \"previous result data\" as indicated by the\n  // `originalData` argument in readRoot(). This behaviour isn't used\n  // for readSelection() however, which always produces results from\n  // scratch\n  const data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, input)\n      : readSelection(ctx, rootKey, rootSelect, input);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: ctx.partial || !data,\n    data: data || null,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  input: Data\n): Data => {\n  const typename = ctx.store.rootNames[entityKey]\n    ? entityKey\n    : input.__typename;\n  if (typeof typename !== 'string') {\n    return input;\n  }\n\n  const iterate = makeSelectionIterator(entityKey, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasChanged = false;\n  const output = makeData(input);\n  while ((node = iterate())) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = input[fieldAlias];\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (node.selectionSet && fieldValue !== null) {\n      dataFieldValue = readRootField(\n        ctx,\n        getSelectionSet(node),\n        ensureData(fieldValue)\n      );\n    } else {\n      dataFieldValue = fieldValue;\n    }\n\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== fieldValue;\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue!;\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n\n  return hasChanged ? output : input;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: Link<Data>\n): Link<Data> => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    let hasChanged = false;\n    for (let i = 0, l = originalData.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the root field's value\n      newData[i] = readRootField(ctx, select, originalData[i]);\n      hasChanged = hasChanged || newData[i] !== originalData[i];\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return hasChanged ? newData : originalData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    return readSelection(ctx, entityKey, select, originalData) || null;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables,\n  fragmentName?: string\n): Data | null => {\n  const fragments = getFragments(query);\n\n  let fragment: FragmentDefinitionNode;\n  if (fragmentName) {\n    fragment = fragments[fragmentName] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'readFragment(...) was called with a fragment name that does not exist.\\n' +\n          'You provided ' +\n          fragmentName +\n          ' but could only find ' +\n          Object.keys(fragments).join(', ') +\n          '.',\n        6\n      );\n\n      return null;\n    }\n  } else {\n    const names = Object.keys(fragments);\n    fragment = fragments[names[0]] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'readFragment(...) was called with an empty fragment.\\n' +\n          'You have to call it with at least one fragment in your GraphQL document.',\n        6\n      );\n\n      return null;\n    }\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename)\n    entity.__typename = typename;\n  const entityKey = store.keyOfEntity(entity as Data);\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeData()) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  input: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  const iterate = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let hasFields = false;\n  let hasPartials = false;\n  let hasChanged = typename !== input.__typename;\n  let node: FieldNode | void;\n  const output = makeData(input);\n  while ((node = iterate()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    if (fieldName === '__typename') {\n      // We directly assign the typename as it's already available\n      dataFieldValue = typename;\n    } else if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, output, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        output[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        output,\n        fieldArgs || ({} as Variables),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          dataFieldValue,\n          ownsData(input)\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (!node.selectionSet) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        (output[fieldAlias] !== undefined\n          ? output[fieldAlias]\n          : input[fieldAlias]) as Data,\n        resultValue,\n        ownsData(input)\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          ownsData(input)\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (dataFieldValue === undefined && deferRef.current) {\n      // The field is undelivered and uncached, but is included in a deferred fragment\n      hasFields = true;\n    } else if (\n      dataFieldValue === undefined &&\n      ((store.schema && isFieldNullable(store.schema, typename, fieldName)) ||\n        !!getFieldError(ctx))\n    ) {\n      // The field is uncached or has errored, so it'll be set to null and skipped\n      hasPartials = true;\n      dataFieldValue = null;\n    } else if (dataFieldValue === undefined) {\n      // If the field isn't deferred or partial then we have to abort\n      ctx.__internal.path.pop();\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = hasFields || fieldName !== '__typename';\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== input[fieldAlias];\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue;\n  }\n\n  ctx.partial = ctx.partial || hasPartials;\n  return isQuery && hasPartials && !hasFields\n    ? undefined\n    : hasChanged\n    ? output\n    : input;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField,\n  skipNull: boolean\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const data = new Array(result.length);\n    let hasChanged =\n      !Array.isArray(prevData) || result.length !== prevData.length;\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        prevData != null ? prevData[i] : undefined,\n        result[i],\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childResult === undefined && _isListNullable);\n        data[i] = childResult != null ? childResult : null;\n        hasChanged = hasChanged || data[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? data : prevData;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (skipNull && prevData === null) {\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || makeData()) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  skipNull: boolean\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const newLink = new Array(link.length);\n    let hasChanged =\n      !Array.isArray(prevData) || newLink.length !== prevData.length;\n    for (let i = 0, l = link.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the link\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined,\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childLink === undefined && _isListNullable);\n        newLink[i] = childLink || null;\n        hasChanged = hasChanged || newLink[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? newLink : (prevData as Data[]);\n  } else if (link === null || (prevData === null && skipNull)) {\n    return null;\n  }\n\n  return readSelection(ctx, link, select, (prevData || makeData()) as Data);\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  Operation,\n  RequestPolicy,\n  CacheOutcome,\n  makeOperation,\n} from '@urql/core';\n\n// Returns the given operation result with added cacheOutcome meta field\nexport const addCacheOutcome = (\n  operation: Operation,\n  outcome: CacheOutcome\n): Operation =>\n  makeOperation(operation.kind, operation, {\n    ...operation.context,\n    meta: {\n      ...operation.context.meta,\n      cacheOutcome: outcome,\n    },\n  });\n\n// Copy an operation and change the requestPolicy to skip the cache\nexport const toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => {\n  return makeOperation(operation.kind, operation, {\n    ...operation.context,\n    requestPolicy,\n  });\n};\n","import {\n  Exchange,\n  formatDocument,\n  makeOperation,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  fromArray,\n  mergeMap,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { addCacheOutcome, toRequestPolicy } from './helpers/operation';\nimport { filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\nimport { Data, Dependencies, CacheExchangeOpts } from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype ResultMap = Map<number, Data | null>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Map<string, Operations>;\n\nexport const cacheExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts?: C\n): Exchange => ({ forward, client, dispatchDebug }) => {\n  const store = new Store<C>(opts);\n\n  if (opts && opts.storage) {\n    opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const operations: OperationMap = new Map();\n  const results: ResultMap = new Map();\n  const blockedDependencies: Dependencies = new Set();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = new Map();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep of dependencies.values())\n      if (blockedDependencies.has(dep)) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: undefined | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep of dependencies.values()) {\n        const keys = deps.get(dep);\n        if (keys) for (const key of keys.values()) pendingOperations.add(key);\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    for (const key of pendingOperations.values()) {\n      if (key !== operation.key) {\n        const op = operations.get(key);\n        if (op) {\n          operations.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    }\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.kind === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.kind === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      operations.delete(operation.key);\n      results.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.kind === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (dependencies.size) {\n        // Update blocked optimistic dependencies\n        for (const dep of dependencies.values()) blockedDependencies.add(dep);\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return makeOperation(\n      operation.kind,\n      {\n        key: operation.key,\n        query: formatDocument(operation.query),\n        variables: operation.variables\n          ? filterVariables(\n              getMainOperation(operation.query),\n              operation.variables\n            )\n          : operation.variables,\n      },\n      operation.context\n    );\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep of dependencies.values()) {\n      let depOps = deps.get(dep);\n      if (!depOps) deps.set(dep, (depOps = new Set()));\n      depOps.add(op.key);\n      operations.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const result = query(store, operation, results.get(operation.key));\n    const cacheOutcome: CacheOutcome = result.data\n      ? !result.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    results.set(operation.key, result.data);\n    updateDependencies(operation, result.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: result.data,\n      dependencies: result.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.kind === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    }\n\n    reserveLayer(\n      store.data,\n      operation.key,\n      operation.kind === 'subscription' || result.hasNext\n    );\n\n    let queryDependencies: void | Dependencies;\n    let data: Data | null = result.data;\n    if (data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(store, operation, data, result.error, key)\n        .dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(\n        store,\n        operation,\n        operation.kind === 'query' ? results.get(operation.key) || data : data,\n        result.error,\n        key\n      );\n\n      data = queryResult.data;\n      if (operation.kind === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n        results.set(operation.key, result.data);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      sharedOps$,\n      filter(op => {\n        return (\n          op.kind === 'query' && op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      sharedOps$,\n      filter(op => {\n        return (\n          op.kind !== 'query' || op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          blockedDependencies.clear();\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import { pipe, merge, makeSubject, share, filter, tap } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  Exchange,\n  ExchangeIO,\n  CombinedError,\n  createRequest,\n  makeOperation,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport {\n  SerializedRequest,\n  OptimisticMutationConfig,\n  Variables,\n  CacheExchangeOpts,\n} from './types';\n\nimport { cacheExchange } from './cacheExchange';\nimport { toRequestPolicy } from './helpers/operation';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = <T extends OptimisticMutationConfig>(\n  config: T,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || {};\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts: C\n): Exchange => input => {\n  const { storage } = opts;\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const { forward: outerForward, client, dispatchDebug } = input;\n    const { source: reboundOps$, next } = makeSubject<Operation>();\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: Operation[] = [];\n\n    const updateMetadata = () => {\n      const requests: SerializedRequest[] = [];\n      for (let i = 0; i < failedQueue.length; i++) {\n        const operation = failedQueue[i];\n        if (operation.kind === 'mutation') {\n          requests.push({\n            query: print(operation.query),\n            variables: operation.variables,\n          });\n        }\n      }\n      storage.writeMetadata!(requests);\n    };\n\n    let isFlushingQueue = false;\n    const flushQueue = () => {\n      if (!isFlushingQueue) {\n        isFlushingQueue = true;\n\n        for (let i = 0; i < failedQueue.length; i++) {\n          const operation = failedQueue[i];\n          if (operation.kind === 'mutation') {\n            next(makeOperation('teardown', operation));\n          }\n        }\n\n        for (let i = 0; i < failedQueue.length; i++)\n          client.reexecuteOperation(failedQueue[i]);\n\n        failedQueue.length = 0;\n        isFlushingQueue = false;\n        updateMetadata();\n      }\n    };\n\n    const forward: ExchangeIO = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.kind === 'mutation' &&\n            isOfflineError(res.error) &&\n            isOptimisticMutation(optimisticMutations, res.operation)\n          ) {\n            failedQueue.push(\n              incomingMutations.get(res.operation.context._instance as []) ||\n                res.operation\n            );\n            updateMetadata();\n            return false;\n          }\n\n          if (res.operation.kind === 'mutation' && !res.error) {\n            incomingMutations.delete(res.operation.context._instance as []);\n          }\n\n          return true;\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations) {\n        for (let i = 0; i < mutations.length; i++) {\n          failedQueue.push(\n            client.createRequestOperation(\n              'mutation',\n              createRequest(mutations[i].query, mutations[i].variables)\n            )\n          );\n        }\n\n        flushQueue();\n      }\n    });\n\n    const cacheResults$ = cacheExchange({\n      ...opts,\n      storage: {\n        ...storage,\n        readData() {\n          return storage.readData().finally(flushQueue);\n        },\n      },\n    })({\n      client,\n      dispatchDebug,\n      forward,\n    });\n\n    const incomingMutations = new WeakMap<[], Operation>();\n    return ops$ => {\n      const sharedOps$ = pipe(\n        ops$,\n        tap(operation => {\n          if (operation.kind === 'mutation') {\n            incomingMutations.set(operation.context._instance as [], operation);\n          }\n        }),\n        share\n      );\n\n      const opsAndRebound$ = merge([reboundOps$, sharedOps$]);\n\n      return pipe(\n        cacheResults$(opsAndRebound$),\n        filter(res => {\n          if (res.operation.kind === 'query' && isOfflineError(res.error)) {\n            next(toRequestPolicy(res.operation, 'cache-only'));\n            failedQueue.push(res.operation);\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n  }\n\n  return cacheExchange(opts)(input);\n};\n"],"names":["wonka","require","getName","node","getFieldAlias","value","getFragmentTypeName","typeCondition","name","isFieldNode","kind","Kind","getFieldArguments","selectionSet","selections","emptySelectionSet","length","i","args","arguments","graphql","valueFromASTUntyped","arg","vars","input","variableDefinitions","l","normalizeVariables","def","variable","key","warn","condition","message","code","cache","console","getMainOperation","Set","doc","definitions","shouldInclude","directive","fragments","directives","isDeferred","buildClientSchema","__schema","argument","field","getField","schema","typename","fieldName","type","ofType","BUILTIN_NAME","indexOf","isInterfaceOfType","getTypeCondition","types","has","get","expectObjectType","invariant","entityKey","parentKey","fieldInfoOfKey","fieldKey","parenIndex","makeDict","Object","currentOwnership","currentDataMapping","slice","currentOperation","parse","currentOptimisticKey","currentOptimistic","dotIndex","replace","ownsData","data","initDataState","WeakSet","currentData","makeData","newData","optimisticOrder","splice","layerKey","isOptimistic","delete","getCurrentDependencies","createLayer","persistData","clearDataState","noopDataState","defer","storage","deferredKeys","keymap","entity","currentDependencies","undefined","set","newCount","link","updateRCForLink","map","optimistic","skip","commutativeKeys","extractNodeFields","count","refCount","by","gc","Array","isArray","fieldInfos","seenFieldKeys","extractNodeMapFields","batch","keys","refLock","readLink","base","links","writeRecord","linkNode","updateDependencies","writeLink","queryRootKey","add","joinKeys","records","setNode","index","Map","prevLinkNode","prevLink","updatePersist","reserveLayer","hasNext","clearLayer","entries","deleteLayer","inspectFields","entry","keyMap","x","_entry","readRecord","_fieldKey","previousDependencies","hydrateData","persist","core","ctx","parent","__typename","clear","contextRef","deferRef","getFieldError","__internal","path","errorMap","join","isFragmentHeuristicallyMatching","error","variables","parentTypeName","parentFieldKey","makeSelectionIterator","next","graphQLError","graphQLErrors","updateContext","select","pushDebugNode","fragmentNode","keyOfField","hasField","childIterator","childDeferred","ensureData","current","_node","store","operation","getSelectionSet","makeContext","_link","fragment","write","request","result","dependencies","rootFields","writeSelection","isRoot","isQuery","iterate","writeField","fieldArgs","InMemoryData","fieldValue","updater","indexKey","childKey","invalidateEntity","constructor","push","mutationName","mutation","query","keyOfEntity","__init","call","this","opts","rootNames","buildType","buildNameMap","arr","fields","interfaces","abstractType","possibleTypes","queryType","abstract","possible","possibleType","isFieldNullable","isListNullable","out","queryName","_id","subscriptionName","subscription","updates","resolveFieldByKey","readQuery","readFragment","id","argsOrLink","__init2","invalidate","read","rootKey","createRequest","output","popDebugNode","formatDocument","partial","fragmentName","dataToWrite","readRoot","maybeLink","dataFieldValue","readRootField","getFragments","pop","fieldAlias","originalData","hasChanged","process","readSelection","_key","hasFields","childResult","resolveResolverResult","prevData","skipNull","_isListNullable","childLink","resolveLink","outcome","meta","_data","context","cacheExchange","newLink","deps","results","values","dep","isDataOrKey","addCacheOutcome","makeOperation","requestPolicy","readData","then","deserializeKeyInfo","operations","writeOptimistic","executePendingOperations","pendingOperations","requestedRefetch","cacheOutcome","blockedDependencies","filterVariables","queryResult","depOps","op","operationResultFromCache","optimisticKeysToDependencies","writeDependencies","queryDependencies","extensions","sharedOps$","share","ops$","cacheOps$","filter","nonOptimisticResults$","res","fromArray","prepareForwardedOperation","merge","nonCacheOps$","cacheMissOps$","cacheResult","updateCacheWithResult","result$","mutationResultBuffer","empty","shift","failedQueue","isOfflineError","response","navigator","onLine","test","networkError","onOnline","readMetadata","writeMetadata","forward","updateMetadata","makeSubject","flushQueue","print","mutations","requests","isFlushingQueue","reexecuteOperation","_i","config","incomingMutations","_instance"],"mappings":"mFAaAA,EAAAC,QAAA,SACAC,EAAAC,QAAAA,WAMAC,KAAAD,OAAAE,MAKAC,EAAAH,GAAAA,EAAAI,cAAAC,KAAAH,MAWAI,EAAAN,GAAAA,EAAAO,MAAAC,EAAAA,MAAAN,MAAAF,EAAAK,KAAAH,WCzBAO,EAAAT,GAAAA,EAAAU,aAAAV,EAAAU,aAAAC,WAAAC,uDAMIN,EAAAN,GAAAA,SAAAa,EAAAL,KAAAM,qDAMG,IAAAC,EAAA,KAEJ,GAAAf,EAAAgB,oDACD,MAAAhB,EAAAgB,UAAAF,GAfFZ,EAAAe,EAAAC,oBAAAC,EAAAjB,MAAAkB,iBAmBAL,EAAA,IAIEA,EAAAhB,EAAAoB,2BAUA,GAAAE,GAAArB,EAAAsB,oBAAA,CASA,IALF,SAKED,EAAY,EAAAE,IAAAD,oBAAAT,OAAAC,EAAAS,EAAAT,IAAA,iEAaZU,EAAA,CAAAxB,EAAAqB,KACE,MAAA,GACD,IAAAA,EAAA,OAAAD,EAED,GAAApB,sBAtBF,IAAA,IAAAc,EAAA,EAAAS,EAAAvB,EAAAsB,oBAAAT,OAAAC,EAAAS,EAAAT,IAAA,gCChDAT,EAAAN,EAAA0B,EAAAC,0FA2CA,aAAAL,OAEAD,IAAAA,EAAAO,GAAAN,EAAAM,IAEE,UAsCF,SAAAC,EAAAC,EAAAC,EAAAC,GACE,IAAAC,EAAAF,CACEG,iBAAAH,GAAA,kBAAAC,EAAA,2CCvEJG,EAAAA,KAAAA,mBAGEpB,ODmCOqB,IClBP,MAAAC,kGAEE,OAAA7B,EAAA8B,YAAAvB,eASJwB,EAAAF,iDAOI,IAAA/B,EAAAN,cAAAwC,oDAWC,OAAAC,GAMLF,EAAA,CAAAtC,EAAAoB,KACA,UAAA,EAAApB,EAAAyC,YAAA3B,EAAAd,EAAAyC,WAAA5B,OAAAC,IAAA,CAIE,MAAAd,EAAAyC,WAAA3B,iLAaO,OAAA,GAKN4B,EAAA,CAAA1C,EAAAoB,0DAED,IAAAmB,EAAAvC,EAAAyC,WAAA3B,GClDF6B,GAAAA,UD0BA5C,EAAAwC,GC1BA,CAAAK,IAAAA,IAAAA,EAAAA,EAAAA,EAAAA,WAAAA,EAAAA,EAAAA,UAAAA,OAAAA,IAAAA,CAAA,IAAAC,EAAAN,EAAAvB,UAAAF,0BAQIG,EAAAC,oBAAA2B,EAAA3C,MAAAkB,aAKG,OAAA,eCAJ,IAAA0B,EAAAC,EAAAC,EAAAC,EAAAC,oDAdH,IAAAJ,EAAAC,EAAAC,EAAAC,EAAAC,kBAqBAH,IAAAA,eAAAD,EAAAE,KAAAzC,KAAAuC,EAAAK,KAAAC,OAAAN,EAAAK,KAKE,MAAAE,SAAAD,EAAAE,mBAAAD,EAAAD,OAEE7C,MAPJgD,OAAAN,oBAeIrB,IAAAA,EAAA4B,EAAAxD,GAfJ,OAAAI,GAAA6C,IAAA7C,MA2BEqD,MAAAC,IAAAtD,IAAA,WAAA4C,EAAAS,MAAAE,IAAAvD,GAAAG,KA3BFH,IAAA6C,GAkFI,SAAAD,sGA1CJY,EAAAZ,EAAAC,uBAaAF,EAAA,CAAAC,EAAAC,EAAAC,sBAlEI,qBAAA,MAoFJ,OAhBAU,EAAAZ,EAAAC,KAIEQ,sBACEP,oBAmBDW,EAAAb,EAAAS,MAAAC,IAAAT,IAAA,WAAAD,EAAAS,MAAAE,IAAAV,GAAA1C,KAAA,GAAA,uDChHDuD,EAAA,CAAAC,EAAApC,IAAA,GAAAoC,KAAApC,IAAAqC,EAAAC,IAJF,IAAAC,EAAAD,EAAAX,QAAA,KC7BAa,OAAAA,GAAAC,GC2DAC,WACAC,UAAAA,EAAAC,MAAA,EAAAL,GACAM,UAAAA,KAAAC,MAAAR,EAAAM,MAAAL,EAAA,GAAA,KAGAQ,CACAC,WAEAzB,UAAAe,qBAKItC,IACA2C,IAAAA,EAAAA,EAAAA,aAGD,MAAA,WAFM3C,EAAA4C,MAAA,EAAAK,GAAAC,QAAA,OAAA,6BAUTC,EAAAC,IAAAX,OAAAW,OAAAV,aAIAW,EAAA,KAMER,EAAAS,KACAC,EAAA,mBAGAP,GAAA,EAEAQ,UACEC,wFAoBIL,EAAAM,KAAAA,GAAAC,EAAA5B,IAAAqB,iBAIFA,EAAA,IAAAE,2DArCNP,EAAAa,EAuDAC,GAAAT,EAAAM,gBAAAxE,OAAA,wBAGE6C,IAAA6B,GAECC,0HAQDD,EAAAA,gBAAAE,OAAAF,IATEG,GAAAA,EAAAA,OAoBCC,GAAAZ,EAAAQ,oCAkBCK,IAAAA,EAAAA,EACAC,EAAAA,EA1CN,QA4CInB,OA5CJa,GAAAR,EAAAM,gBAAA/B,QAAAiC,IAAA,MAiDAO,IAAAA,EAAAA,EAAAT,gBAAAxE,4IAOEgF,GAAAd,EAAAM,gBAAAvE,IAYAuD,OATFC,EAAA,KAYAE,EAAA,YACAkB,EAAAA,KAYAX,EAAAgB,QAAAhB,EAAAgB,OAAA,qBAAA,OAAAhB,EAAA,mBAAAA,EAAAgB,OAAA,OAiBEV,EAjBF,CAAAN,EAAAQ,EAAAC,KAkBEQ,IAAAR,GAAAT,EAAAkB,aAAAR,OAAAF,GAlBFP,EAAA,QAAAD,EAAAQ,EAAAC,GAqBAK,KAgBIK,EAAAC,KAfJtC,EAAA,OAAAuC,EAAA,GAAA,QA0DMpG,CAAAA,EAAAiE,EAAAA,EAAA/D,wDA7BNmG,IAAAF,GAsCAD,EAAAI,IAAAxC,EAAAqC,EAAAhC,UASEkC,IAAAE,GAAA7B,gBACA4B,kBAmBE,wDAAAxF,IAAAS,EAAAiF,EAAA3F,gBAAAA,OAAAC,EAAAS,EAAAT,IAAA,4BAEI2F,EAAAC,EAAAC,WAAAhD,IAAA4B,GAIH,KAFGqB,GAAArB,IAAAb,EAEHiC,KAAAC,IAAA1B,EAAA2B,gBAAAnD,IAAA6B,OAAAZ,GAAA,UAAAH,GAAAU,EAAA2B,gBAAAnD,IAAA6B,UAAAc,KAAArG,EAAA2G,EAAAhD,IAAAG,KAAAG,KAAAjE,EACF,OAAAA,EAAAiE,oBAIH6C,EAAAA,EAAAA,KAAAA,mCAYO,IAAAC,EAAAC,EAAArD,IAAAG,IAAA,EACFyC,EAAAQ,EAAAE,EACFD,EAAAV,IAAAxC,EAAAyC,8CAeCE,EAAA,CAAAS,IAAAvD,EAAAuB,2CAEE,GAAAiC,MAAAC,QAAAZ,GACD,IAAA,IAAA1F,EAAA,EAAAS,EAAAiF,EAAA3F,OAAAC,EAAAS,EAAAT,IACFqG,MAAAC,QAAAZ,EAAA1F,IAfH2F,EAAAS,EAAAF,EAAAR,EAAA1F,GAAAmG,GAkBAT,EAAA1F,mBAKEgG,EAAA,CAAAO,EAAAC,EAAAtH,mBACA,QAAAiE,iCAIIH,IAAAA,KAOAyD,EAAA,CAAAF,EAAAC,EAAAxD,EAAA4C,8EAKEM,EAAAvB,EAAA3B,WAAAH,IAAAuB,EAAAG,gBAAAvE,SAhBNuF,IAAAM,qBAyBEO,OAGIT,IACDS,GAAAM,GACFtC,EAnCL,IAAA,IAAApB,KAAA0D,EAAAC,OAAA,CAyCI,MAFJT,SAAArD,QAAA,GAEI,cACEyC,YAHN,IAAA,IAAAb,KAAAL,EAAAwC,QAAAD,OAAA,wBAWE,GAAA9C,EAAAA,CAKF,IAJIqC,EAAArD,IAAAG,IAAA,GAIJ,EAAA,oBACAoB,EAAA8B,SAAAvB,OAAA3B,GAQA0D,EAAA/B,OAAA3B,4BACA6D,MAAAzC,QAAA0C,KAAAjE,IAAAG,qBAAAoB,EAAA2C,MAAAD,KAAAnC,OAAA3B,KASAgE,EAAAhE,EAAAoB,WAAA6C,EAAA9D,IAAA,KAcA+D,EAAA,CAAAlE,EAAAG,wBACAgE,IAAAnE,EAAAoE,eAAAC,IAAArE,eAOEkD,EAPFmB,IAAAC,EAAAtE,EAAAG,oBAYEiB,EAAAc,uBAxaE,EAAAlC,MAAiCiB,GAAAjB,EAAAe,QAAA,MAAA,UAAAZ,YAgbjC+C,EAAA,CAAAlD,EAAAG,KACA4D,EAAA/D,EAAAG,GACAc,EAAAG,EAAAmD,QAAAvE,EAAAG,yBAQFH,MAAAG,EAAAA,8BA9BFqE,EAAApD,EAAAmD,QAAAvE,EAAAG,EAAA/D,IAiDI6E,GAAA,CAAAjB,EAAAG,EAAAuC,KACD,MAGD+B,IAHCxD,EAAAG,2BAWEH,EAAA2C,QAAApB,IAAA5B,EAAAsC,EAAA,IAAAwB,KApBLX,EAAA9C,EAAA8C,MAAAlB,WAAAhD,IAAAe,iCAyBE6D,EAAAA,EAAArB,IAWAnC,IAAA0D,EAAAZ,GAAAtC,EAAA5B,IAAAG,GApCF4E,EAAAD,GAAAA,EAAAxE,UAwCA0E,EAAA7E,KAEIiB,EAAAA,EAAA8C,MAAA/D,EAAAyB,EAAAiB,2BAUJoC,GAAA,CAAA7D,EAAAQ,EAAAsD,yFAOG,GAAAN,GAAA,EAAA,CANH,IAAAM,GAAA9D,EAAA8B,gBAAAnD,IAAA6B,UASAR,EAAAM,gBAAAC,OAAAiD,EAAA,MACAxD,EAAAQ,GAOEuD,MAAA/D,EAAAQ,GAAAgD,EAAAxD,EAAAM,gBAAAxE,SAAAkE,EAAAkB,aAAAvC,IAAAqB,EAAAM,gBAAAkD,OAAAxD,EAAA2C,QAAAhE,IAAAqB,EAAAM,gBAAAkD,MAAAxD,EAAA8B,gBAAAnD,IAAAqB,EAAAM,gBAAAkD,KAAAA,KAGFxD,EAAAM,gBAAAC,OAAAiD,EAAA,EAAAhD,6BAIEI,GAAA,CAAAZ,EAAA5C,MAEAoD,IAAAsC,EAAAxC,gBAAAwC,QAAAlB,mCAGIe,QAAAhE,SACAqB,UAAAuB,MAAA,IAAAkC,6EAMJH,GAAAnD,CAAAA,EAAAA,wBACAH,UAAAU,OAAAF,GACER,EAAAsD,QAAA1B,kBAAAoC,GACEhE,QAAA4B,kBAAApB,GACAR,sBAAAQ,KAIHyD,GAAA,CAAAjE,EAAAQ,2CAGDL,IA1BFH,EAAAM,gBAAAC,OAAAiD,EAAA,GA6BAxD,EAAA8B,gBAAApB,OAAAF,IACA0D,GAAAA,EAAAnF,4FAUE,MAAAoF,EAAA,GAVFC,EAAAD,EAAA,gGAoBM,IAAAE,EAAAC,EAAA,yBAGOvB,EAAAwB,EAAAC,EAAAtF,EAAAA,IAKRmC,EAAAoD,WAfLP,GAAAnF,IAuBA2F,IAAAA,MAKEzE,EAAAA,WAEAE,iBAII,qCAIEU,QAED,GAAAV,EAAAc,QAAA,CACFrB,GAAA,EACFH,EAAA,iBAGD,IAAA,IAAA7C,OAAA+H,QAAAjC,OAAA,CAtBF,IAAA3D,0BCpmBAnC,GAAAyH,OAAA,YACA/C,KAAA+C,EAAAzB,EAAA7D,EAAAG,sDAGA0F,EAAAC,mBAAA/I,QAcEwF,EAIEwD,GAAA,EAAAC,EAAA9D,QAAA/C,UAAAA,GAJFiC,EAAAwE,QAAAK,UAwCAC,GAAA,CACAJ,QAAA,MAEAK,GAAA,CACAL,SAAA,GAZFM,GAAAN,GAAAA,EAAAO,WAAAC,KAAAvJ,OAAA,GAAA+I,EAAAO,WAAAE,SAAAT,EAAAO,WAAAE,SAAAT,EAAAO,WAAAC,KAAAE,KAAA,WAAAjE,EAiBAkE,GAAAA,CAAAA,EAAAA,EAAA/H,IAAAsB,EAAA1C,EAAAoJ,KAME,IAAAZ,EAAA3G,SAEAwH,gCAmBIX,WAAA7G,GAEAyH,eAAAzH,cA7BN0H,eAAA,gBAsCAC,WAAAA,aAOEjE,2BAGAyD,KAAAS,GACER,cAAAhE,oEAKI,IAAAyE,EAAAN,EAAAO,cAAAjK,yIAQH,OAAA8I,GAGCoB,GAAAC,CAAAA,EAAAlG,EAAA9B,EAAAa,EAAAG,EAAAf,gDAGC0G,EAAA7F,UAAMzD,qBAELsJ,EAAA1G,0CAaE,IAAAD,SAAA,EACE,IAAA7C,EAAAoD,KACE0H,OAAAA,OAAAC,GDxFH,UAAA3F,2CC4FC4F,EAAArL,EAAAC,GAAAS,EACET,EAAAoB,IAEF,ODyTZwH,EAAA9E,EAAAG,SAAAoC,MAAAvC,EAAAG,SAAAoC,IAAAsB,EAAA7D,EAAAG,GCzTYoH,CAAAC,EAAAA,OASJV,GAAA,CAAA3H,EAAAa,EAAAmH,EAAArB,KACD,IACF0B,EADEC,GAAA,EAtDLhD,EAAA,EAVF,OAAA,eAqEAiD,GAAAC,SAAAF,IAAAtB,GAAAwB,QAAAF,MAKI,IAAAvL,EAAAsL,wBAGAA,OAAAjF,yBAOAzE,IAAA8J,SAZJ,GAAApJ,EAAAoJ,EAAA9B,EAAAa,WAAA,CAqBE,GAAAnK,EAAAoL,GCrKA7F,OAAAA,iDA7BF+D,EAAA7E,MAAAyF,OAAAjH,EAAAqG,EAAA+B,MAAA3I,OAAAmI,EAAAlI,GAAAsH,GAAAY,EAAAlI,EAAAa,EAAA8F,EAAAa,YAyBG,aAPDiB,EAAA9B,EAAAa,YACER,GAAAwB,SAAAF,IAAAK,GAAAH,QAAAF,IAMDD,EAAAV,GAAA3H,EAAAa,EAAA+H,EAAAV,GAAAvB,qDAmBDA,OAAAkC,uBAYC,IAFD,UAAA3E,gBAECrG,EAAA,EAAAS,EAAAwK,EAAAlL,OAAAC,EAAAS,EAAAT,oBAID,SAiBEkL,yBAaCC,GAAA,CAAAN,EAAAO,EAAAnH,EAAAyF,EAAA7I,KACFqD,EAAM,QAAA2G,EAAA5G,KAAApD,GAAA,wEAyBNwK,EAAA,QAEDC,aAAA1G,KAECnF,EAAAoL,EAAAU,WAAAT,EAAAA,yDA0BD,OAXCU,GAAA1C,EAAArJ,EAAAsL,EAAAD,GAAA7G,GAWD9B,GAoBAqJ,GAAA,CAAA1C,EAAA9F,EAAAmH,EAAAlG,qEA0DE9B,EAAAsJ,GAAAC,EAAA1I,EAAAiB,EAAA+E,WAEEkB,GAAA/H,EAAA+H,+BAeC,QAPChL,mBAOKA,EAAAyM,KAAA,CACLC,IAAAA,EAAA3M,KACD4M,EAAAlM,EAAAT,EAAA4J,EAAAa,aACI3G,IAAA6I,UAELC,EAAA7H,EAAA6E,EAAAjD,WAAA1C,EAAA4I,GAyBEC,KACD,eAAA5J,QA1GLmD,IAAAwG,GAAA5C,GAAAwB,SAyGMqB,2CAkBNlD,EAAAxC,YAAAmF,0DAEE5F,YAAA,mBAAAkG,SAYEjD,OAJAoB,GAAApB,EAAA7E,IAAAA,EAAAgI,EARF7J,6BAYE0G,EAAAlJ,uBAGF,MAAA0H,EAAAtE,EAAAG,wBAEAgE,MAAA5B,EAAApC,EAAAuC,8BAIF1C,OA7BFgE,EAAAhE,KAAAG,EAAA,OAAA4I,GAAA3C,GAAAN,QAAAvD,EAAAwG,GAqDG,GAAAN,EAAA,CAEDS,IAAAF,EAAAlD,EAAA+B,MAAAhB,WAAAzH,4BClYF+J,EAAAA,GAAAnJ,SAMI,GAAA8F,EAAA+B,MAAA/B,+CASC,GAAAzC,MAAAC,QAAArC,GAAA,KACF,IAAAK,EAAA,IAAA+B,MAAApC,EAAAlE,gCC6BDqM,EAAAA,WAAA9C,KAAA+C,KAAArM,4HAaIsM,MAAAxD,EAAA5G,MAAAqK,YAAAD,MAYAE,mBAAAA,iCA0BJC,GAAAA,UAAAC,OAAAC,KAAAC,gDAOEA,KAAA3I,YAAAA,WAAA,4BACsB4I,cAAA,GACtBD,KAAAjG,KAAAmG,EAAAA,MAAA,GAEA,MAAAjM,EAAA,sCAGC,KAAMoD,OAAA,CACLpD,MRhEF,kBASF,IAAAkM,EAAA1K,IAAAA,IAII2K,EAAAC,IACA,6BAKIC,IAAAA,IAAAF,EAAAA,EAAAA,EAAAA,EAAA3K,OAAA6K,iBAAAD,EAAAjN,uEAVRT,KAAA8C,EAAA9C,4BA2BAyN,EAAA3K,EAAA8K,YAAA,IACED,OAAAF,EAAA3K,EAAA6K,OAAAtH,KAAA5D,IAAA,CACAzC,KAAAyC,EAAAzC,iBAFFU,KAAA+M,EAAAhL,EAAA/B,+BAUIV,KAAA6N,EAAAA,KACE3N,OAAAA,WACK2N,EAAA3N,EAAA4N,eAAA,UASNb,MAAA1K,EAAAwL,UAAAxL,EAAAwL,UAAA/N,KAAA,mHArBLoD,WAAA4C,iBA0BE,IAAA6H,IAAAvK,IAAA0K,8BAGE,UAAAH,EAAA3N,KACE2N,EAAA/K,QAAAmL,wCAEDC,EAAAN,aAAAI,GAEJA,IAAAC,MC9GHE,GAAAA,EAAAA,MAAA,WAAA,IAAA,IAAA1N,EAAA,EAAAA,EAAA8B,EAAAa,MAAA5C,OAAAC,IAAA,CASA2N,IAAAA,EAAAA,EAAAzL,MAAAlC,GAME,GAAAgC,GAAYK,OAAA,YAGZC,GAAA7C,EAAA+F,IAAAnD,EAAA9C,KAAAqO,KAkBA,OAAA1L,GOoEIL,CAAAgL,EAAA3K,UACK+B,EAAAuI,OAAAqB,EACLhN,EAAAiN,EAAAvB,UAAAD,EACDyB,EAAA7L,EAAA8L,cAAAD,IAEDpL,QAAAiK,KAAA5D,eAGFiF,QAAA,0JAUA,YAAAC,WAAAH,CAAAA,GAAA,6BAEA/L,GJoIAiD,OAAA,KACEI,IAAAA,IACDuD,QAAM,IAAAvH,IACLgE,eACDa,SAAA,IAAAwB,IAzBHd,QAAA,IAAAc,IA4BAX,MAAA,kCAMEQ,QALF,uDAcExB,gBAAA,IAAA1E,gFI3HG,OAAA6H,GAAAyB,SAAA1G,IAAAiF,GAAAyB,QAAA5B,OAAAG,GAAAyB,QAAA1H,2DAGHkL,KAAA5N,UAAA0D,EAAA+E,YAAA/E,EAAA+E,4EAMAoF,EAAAA,GAAAlD,EAAAmD,KAME,MAAApK,EAAA6J,oBAeAjN,EAAA,GAAAoD,oBAAAiH,WA1BA,IAAAE,EA4CF1F,QAAAL,IAAAiJ,GAME,IAAArO,EAAAqK,OACA5E,EAAAkH,KAAAH,8CAIA,YAAAlH,IAAAwG,EAAAA,EACElF,EAAA7D,EAAAG,IAKD,KApMLoL,8CC8BEC,WAAAC,EAAA5D,EAAAO,GACArG,IAAAA,EAAA6H,KAAAH,YAAApH,GACAtC,IAAA,GAAA,iID4BGiE,EAAAhE,EAAAG,OAAAoC,KCtCL4G,CAAAnJ,EAAAhB,EAAA/B,wBAqBE+C,mBAAA8H,GAEAhC,OAAAkC,EAAA7C,GAAAzH,GAAAoK,GAWEV,cAAAsE,GACD,IAAAtD,EAAAvC,EAAA8F,cAAApO,EAAAiM,MAAAjM,EAAAoJ,6CAED,IAAAiF,mBAxBFxD,2BAkCE+C,aACEU,IAAAA,EAAAhG,EAAA8F,cAAApO,EAAAiM,MAAAjM,EAAAoJ,kBACDyB,EAAAoB,MAAA3D,EAAAiG,eAAA1D,EAAAoB,uBAICuC,aAAAjG,EAAAzD,wEH0FA,EAAAwF,IAAA/J,EAAA6I,EAAAqF,kBAQD,GAAAA,GASD,aACA9P,OAAA,eAIEgM,EAAAxJ,sBAKE,OAAA+J,KAqBC,IAAAtJ,UAEA6G,WAAA7G,KACF8B,GAEDjB,EAAA6H,EAAA4B,YAAAwC,wCAcAzD,GAAA1C,EAAA9F,EAAA+H,EAAAG,GAAA+D,KGxJJC,CAAAtC,sBAAArM,GAAA0D,EAAA0F,EAAAqF,GASEtJ,KAAAL,EAAArD,EAAAsM,EAAAa,GACE,IAAAlP,OAAAsF,MAAA+I,EAAA,KACD5I,OAAAH,IAAA4J,EAAAA,EAAAb,eAID,iBAAAtL,oGAWEyL,IAAA5D,EAAAjL,IAAA8J,KACE0F,IAAAA,IAAAC,EAAA7C,OAKKkC,EAAA7D,EAAAU,WAAAT,EAAAA,kBAdThC,EAAAkC,GAAAH,EAAAnK,EAAAoK,EAAAM,EAAAzB,WAAA2F,EAAAlE,EAAAoB,OAAAkC,EAAAA,GAAA,EAAAhF,GAuBEnJ,IAAAA,kEAcA,MAAA,0FAOEuI,EAAAQ,MAAAiG,UAAAvM,GAAAA,EAAAzC,EAAAyI,yBAfN,MAAA2C,EAAA7B,GAAA9G,EAAAA,EAAAmH,EAAArB,2CA6BSA,EAAAO,WAAAC,KAAA+C,KAAAmD,GAcPtE,EAFAhM,EAAAU,cAAA,SAEAyP,GAAAvG,EAAAiC,EAAA7L,GAAAwL,GAAAqB,IACAA,IAGIjL,GAAAA,WAUAyE,IAAA6J,IAAAR,EAAAY,GAAAJ,GAEGtG,EAAAO,WAAAC,KAAAiG,oBAUHF,GAAA,CAAAvG,EAAAqB,EAAAsF,KACD,GAAApJ,MAAAC,QAAAmJ,GAAA,KACF,IAAAnL,EAAA,IAAA+B,MAAAoJ,EAAA1P,8DAeCuE,KAAA+K,GAAAvG,EAAAqB,EAAAsF,EAAAzP,IACD0P,EAAAA,GAAApL,EAAAtE,KAAAyP,EAAAzP,GAED2P,aAAArG,wBAIA,GAAAK,OAAA8F,SAQA,KAMC,IAAAzM,EAAA8F,EAAA+B,MAAA4B,YAAAgD,GAED,OAAA,OAAAzM,EAGF4M,GAAA/O,EAAAsJ,EAAA5J,EAAA8K,IAAA,KAOER,GAAAA,EAAAA,EAAAA,WAAAA,EAAAA,qBAPF,IAWEK,EAXFxJ,EAAA4N,KAwBG,GAAAN,QAxBH9D,EAAAxJ,EAAAsN,IA+BE,OAAA7M,mBAGErB,OAAAA,KAAAA,oBAiBF,IAAA8N,IAAA1D,6EAGE,MAEA,OAAA,6EAeA,IAAAkE,+HAqBG/D,GAAAlJ,IAAAkJ,EAAArC,YAAA,mCAwBA0G,EAAAvN,IAAA5B,EAAAyI,qCAUA,IAAA5G,EAAAnD,EAAAC,KACIS,EAAAT,EAAA4J,EAAAa,oCAiBAoC,EAAAvD,EAAAxF,EAAAG,oCAmBJ2F,EAAAO,WAAAC,KAAA+C,KAAAmD,gBAMH,GAAA,eAAAJ,+BAKE7J,MAAA3F,uFAQAkJ,GAAAA,EAAA8F,EAAAzM,EAAAa,EAAA6M,EAAAzN,QAEKmD,IAAAwG,gCAMP7M,EAAAU,uGA9LJ,8BAuNIiL,QAAAA,IAAAA,2EAOAtF,IAAAG,kDAEA1F,iBAAAqL,GAAA,OAAAU,cATAqD,EAAArD,4BA2BI+D,UACK,QAAAvK,IAAA6J,IAAAvE,EAAA3I,QAAAwL,EAAA7C,EAAA3I,OAAAC,EAAAC,IAAAgH,GAAAN,OAGLiH,IACA,SACD,SAAAxK,IAAA6J,EAlCL,oCAyCEU,EAAAA,GAAA,eAAA1N,0BAeAsN,KAAAN,IAAA7O,EAAAiP,QACDjK,IAAA6J,IAAAR,EAAAY,GAAAJ,GAYD,OATFtG,UAAAA,EAAAiG,SAAA5M,EASEuJ,GAAAhG,IAAAoK,OAAAvK,EAAAmK,EAAAd,EAAArO,IACEyP,GAAA,CAAAlH,EAAA3G,EAAAC,EAAAvB,EAAAsJ,EAAA8F,EAAA5E,EAAA6E,0BAWE,IAVF,IAAAC,eAMAzK,EAAA3F,QAAAU,EAAAoK,EAAA3I,OAAAC,EAAAC,kEAIEgO,EAAAA,EAAAC,EAAAA,EAAAA,OAAA3K,EAAAjF,EAAAT,KAUA8I,EAAAO,WAAAC,KAAAiG,QAEA,IAAAa,EAAA7K,OAAAnD,EAAAkF,EAAAzG,EAAA,GAAAb,KAAAmK,EAAA,MAAA8F,EAAAA,EAAAjQ,QAAAuF,EAAA8F,EAAArL,GAAAkQ,MAECpH,EAAMO,WAAAC,KAAAiG,WAILhK,IAAAwK,IAAAI,EACD,8DA/BLT,iBAuCAE,OAAAA,EAAA9G,EAAAmH,EAhDF,GAAA5E,uBAmDA,OAAA/C,uBCnlBAgI,CAMIC,IAAAC,KACEC,IADF,MAAA,iBAAApF,EAAAuE,GAAA9G,EAAAuC,EAAAlB,EAAAqG,GAAAZ,GAAA9G,EAAAjI,EAAAsJ,EAAAqG,EAAAnF,KAOJgF,GAAA,CAAAvH,EAAApD,EAAAvD,EAAAC,EAAA+H,EAAA8F,EAAAC,8BCkBAQ,IAAA7F,MAAAA,GAAA/B,EAAAqH,IAAAtF,EAAA3I,QAAAyL,EAAA9C,EAAA3I,OAAAC,EAAAC,yBAKEsN,SAAApJ,QAAA2J,IAAAU,EAAA5Q,SAAAkQ,EAAAlQ,+BAIC+I,EAAAO,WAAAC,KAAA+C,KAAArM,gDAQD4Q,GAHAC,EAAAxH,WAAA3B,KAAA6H,WAGAhK,IAAA6K,IAAAD,SAGErH,0BAAAvD,IAAAuL,GAAAX,EACEQ,EAAA3Q,GAAAoQ,GAAAW,KADFrB,EAAAA,GAAAiB,EAAA3Q,KAAAiQ,EAAAjQ,gBAKF,OAAA,OAAA0F,GAAA,OAAAuK,GAAAC,yBAQgBc,GAAA1I,GAAA,iBAAAA,GAAA,iBAAAA,GAAA,iBAAAA,EAAAU,WAEbiI,GAAA,CAAAnG,EAAAwF,IAAAzH,EAAAqI,cAAApG,EAAArL,KAAAqL,EAAA,IAAAA,EAAA2F,aAVH,IAAA3F,EAAA2F,QAAAF,wEAqBMY,kBAGET,GAAA7D,GAAA,yCAMHA,GAAAA,EAAA3H,SACF2H,EAAA3H,QAAAkM,WAAAC,MAAApJ,INzBH,EAAAhE,EAAAiB,EAAA+C,kBASEyB,EATF,QAAAzF,EAAA,QAAA,CAYEoF,MAAApB,EAAApH,GAEE0I,QAAAhE,IAAAgE,EAAAhE,CAFF,IAAAvC,UAZFA,EAAAG,YAkBAmO,yEAIM9I,IAAAe,8BAONtF,EAAAiB,QAAAA,GM5DFyD,CAAAkC,EAAA5G,KAAA4I,EAAA3H,QAAA+C,MA6DI,MAAA,IAAAP,qBAGOoD,IAAAA,cAELyG,UACAV,EAAA,IAAAnJ,UAGD,IAAM,IAAAqJ,KAAAzF,EAAAwF,8BAKLU,OAAA,aAGE,GAAAlG,kFAUHmG,EAAA,CAAA3G,EAAA4G,KAED,QAAA7Q,KAAA6Q,EAAAZ,kBAAAjQ,IAAA,KAIIiO,EAAAA,EAAAA,IAAAjO,GAODiK,GAAAA,EAAA,CArGPyG,EAAA5M,OAAA9D,uBA2GE8Q,EAAA/O,kBACEkO,6DAaAhG,IACA,GAAA,UAAA8G,EAAAnS,KAOAyH,GAAAA,EAAAjD,KAAA6G,EAAAQ,sFAjIJ,GAAA,aAAAR,EAAArL,MAAA,iBAAAqL,EAAA2F,QAAAU,cAAA,kBA4IE7F,GLtBI,EAAAT,EAAAO,EAAAvK,oHK2BFgR,EAAAxK,IAAA0J,GAEAjG,EAAAtF,IAAAsF,EAAAjK,IAAAyK,mBAGEoG,EAAApG,OACAzK,IASF,uBAAAiK,EAAArL,KAAA,KACAqL,EAAAjK,oCACA8I,UAAAmB,EAAAnB,UAAAmI,EAAA1Q,EAAA0J,EAAA0B,OAAA1B,EAAAnB,WAAAmB,EAAAnB,uBAOEzC,EAAAsF,CAAAA,EAAAlB,SAQA,IAAAyG,KAAAzG,EAAAwF,SAAA,gBACAkB,GAAApB,EAAApL,IAAAuL,IAAA,IAAA1P,oBAEEmE,IAAAyM,EAAApR,IAAAoR,KAxCNC,EAAApH,yEAmDG,OAFD+F,QAAAhQ,IAAAwK,EAAApH,MACEiD,EAAAA,IAAAoE,cACD,oHAYG2G,UAWAA,0BAiBA,IAAA3G,EAAA6G,EAAAtP,IAAAhC,GACEwB,EADFqP,EAAApG,GAEEtK,EAFF2D,OAAA9D,GAAAiH,GAAA+C,EAAA5G,KAAA6G,EAAAjK,IAAA,iBAAAiK,EAAArL,MAAA4L,EAAAtD,sBAFF,UAaFqK,EAKMjH,GAAAN,EAAAC,EAAA7G,EAAAoH,EAAA3B,MAAA7I,GAAAyK,4GAKAoG,EACE5G,EAAAmG,EAAA3F,oBADFR,EAAAjK,IAAAwK,EAAApH,4BAqBG,OARDoO,oBAQC,CACFpO,eAEDqO,aACEjQ,cAIEjD,OAAAA,IALJ,IAAAmT,EAAAxT,EAAAyT,MAAAC,GAAAC,EAAA3T,EAAAyT,MAAAzT,EAAA6G,IAAAsM,EAAAnT,CAAAA,EAAA4T,QAAAV,GASA,UAAA5G,QAAA,iBAAA4G,EAAAxB,QAAAU,eATApS,CAlCN4T,oFA0DAC,EAAA7T,EAAA6G,KAAAiN,GAAA5B,GAAA4B,EAAA/H,UAAA,SAAA/L,+GAqBMA,EAAA6G,KAAAiN,IACE,IAAA/H,UACDA,EAAAwF,uBAEDhF,GACEuH,EACDxH,EAAA,wDAKDwH,EAAAP,2CAGA7B,QAAAU,eAAA,gBAAArG,EAAA2F,QAAAU,eAAA,YAAAb,qBAMAoB,sBAAAD,EAAAA,QAAAN,eAEA2B,EAAAA,IAAAjC,EAAAhQ,qDAtBA9B,mGC3TCiD,EAAAjD,EAAA6G,IAAAmN,EAAAhU,CAAAA,EAAAiU,MAAA,CAAAC,EAAAC,OAIL7I,aAJK,IAAAqH,EAKA1P,IAAAA,IAENmR,EAAAC,EAAA/H,EAAAqG,GAGH,6BAtBF3S,EAAA4T,QAAAtH,IAAA8G,EAAAvP,IAAAyI,EAAAP,UAAAjK,MAAA9B,CAAAsU,sBAqCEnO,GAHFoO,EAAAjH,KAAAhB,GAGEnG,EAAAA,YAAAnG,EAAAwU,yEASE,2BAAA1G,EAAA2G,wBAKE,wBAAAzU,EAAA+T,UAAAW,MACE1U,EAAA4T,QAAAtH,KAAAzI,IAAAyI,EAAAP,UAAAjK,MAAA9B,CAAAsU,8BAuBAK,GAAAhK,GAAA+J,GAAAA,EAAA1T,eAAA2J,EAAAiK,WAAA,oBAAAC,YAAA,IAAAA,UAAAC,QAAA,kDAAAC,KAAApK,EAAAqK,aAAA/S,4EAAA6L,GAAAtM,qBAMD,GAAA2E,GAAAA,EAAA8O,UAAA9O,EAAA+O,cAAA/O,EAAAgP,cAAA,sBAGHC,EAAAA,iBAIM5T,GASE6T,OAAAA,OACArK,GACDhL,EAAAsV,iCAEDZ,EAAA3I,GAECsJ,EAAA,sCAnBP,IAAAtJ,EAAA2I,EAAAzT,GA0BA,aAAAgU,EAAAM,oBAEEnU,EAAAoU,MAAAzJ,EAAA0B,OACE7C,UAAA3J,EAAAwU,YAUDtP,EAAAgP,cAAAO,IAKDC,GAAA,qBAAA,IAAA,IAAA1U,EAAA,EAAAA,EAAAyT,EAAA1T,OAAAC,IAAA,CAMD,IAAA8K,EAAA2I,EAAAzT,GAAA,aAAA8K,EAAArL,iDAcG+S,EAAAA,EAAAA,eAHEmC,qBAAAC,8SAgBC7V,EAAA4T,QAAAE,+CAvGF,EAAAgC,EAAA/J,SACF,MAAAxK,EAAAwK,EAAAnB,WAAA,gBACDzE,EAAAgP,IAAAA,EAAA9S,EAAA0J,EAAA0B,uCAQE,GAAAqI,EAAA5V,EAAAwU,IACE,sDAGC,OAAA,oBA2FDA,EAAApH,KAAAyI,EAAAjS,IAAAgQ,EAAA/H,UAAA2F,QAAAsE,YAAAlC,EAAA/H,oBAMR,yBAAA+B,MAAAgG,EAAAnJ,OACFoL,EAAAnQ,OAAAkO,EAAA/H,UAAA2F,QAAAsE,gBATWhW"}