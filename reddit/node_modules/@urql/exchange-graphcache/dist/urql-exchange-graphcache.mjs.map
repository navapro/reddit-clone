{"version":3,"file":"urql-exchange-graphcache.mjs","sources":["../src/ast/node.ts","../src/ast/variables.ts","../src/helpers/help.ts","../src/ast/traversal.ts","../src/ast/schema.ts","../src/ast/schemaPredicates.ts","../src/store/keys.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/operations/invalidate.ts","../src/store/store.ts","../src/operations/query.ts","../src/helpers/operation.ts","../src/cacheExchange.ts","../src/offlineExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : node.name.value;\n\nconst emptySelectionSet: SelectionSet = [];\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet ? node.selectionSet.selections : emptySelectionSet;\n\nexport const getTypeCondition = (node: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  node.typeCondition ? node.typeCondition.name.value : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  let args: null | Variables = null;\n  if (node.arguments) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        if (!args) args = {};\n        args[getName(arg)] = value as any;\n      }\n    }\n  }\n  return args;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | Record<string, unknown>\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 14\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25\n  | 26;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import {\n  SelectionNode,\n  DocumentNode,\n  OperationDefinitionNode,\n  FragmentSpreadNode,\n  InlineFragmentNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  for (let i = 0; i < doc.definitions.length; i++) {\n    if (doc.definitions[i].kind === Kind.OPERATION_DEFINITION) {\n      return doc.definitions[i] as OperationDefinitionNode;\n    }\n  }\n\n  invariant(\n    false,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments => {\n  const fragments: Fragments = {};\n  for (let i = 0; i < doc.definitions.length; i++) {\n    const node = doc.definitions[i];\n    if (node.kind === Kind.FRAGMENT_DEFINITION) {\n      fragments[getName(node)] = node;\n    }\n  }\n\n  return fragments;\n};\n\n/** Resolves @include and @skip directives to determine whether field is included. */\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n\n/** Resolves @defer directive to determine whether a fragment is potentially skipped. */\nexport const isDeferred = (\n  node: FragmentSpreadNode | InlineFragmentNode,\n  vars: Variables\n): boolean => {\n  for (let i = 0; node.directives && i < node.directives.length; i++) {\n    const directive = node.directives[i];\n    const name = getName(directive);\n    if (name === 'defer') {\n      for (\n        let j = 0;\n        directive.arguments && j < directive.arguments.length;\n        j++\n      ) {\n        const argument = directive.arguments[i];\n        if (getName(argument) === 'if') {\n          // Return whether `@defer(if: )` is enabled\n          return !!valueFromASTUntyped(argument.value, vars);\n        }\n      }\n\n      return true;\n    }\n  }\n\n  return false;\n};\n","import {\n  IntrospectionQuery,\n  IntrospectionSchema,\n  IntrospectionInputValue,\n  IntrospectionTypeRef,\n  IntrospectionType,\n} from 'graphql';\n\nexport interface SchemaField {\n  name: string;\n  type: IntrospectionTypeRef;\n  args(): Record<string, IntrospectionInputValue | void>;\n}\n\nexport interface SchemaObject {\n  name: string;\n  kind: 'INTERFACE' | 'OBJECT';\n  interfaces(): Record<string, unknown>;\n  fields(): Record<string, SchemaField | void>;\n}\n\nexport interface SchemaUnion {\n  name: string;\n  kind: 'UNION';\n  types(): Record<string, unknown>;\n}\n\nexport interface SchemaIntrospector {\n  query: string | null;\n  mutation: string | null;\n  subscription: string | null;\n  types?: Map<string, SchemaObject | SchemaUnion>;\n  isSubType(abstract: string, possible: string): boolean;\n}\n\nexport interface PartialIntrospectionSchema {\n  queryType: { name: string; kind?: any };\n  mutationType?: { name: string; kind?: any } | null;\n  subscriptionType?: { name: string; kind?: any } | null;\n  types?: IntrospectionSchema['types'];\n}\n\nexport type IntrospectionData =\n  | IntrospectionQuery\n  | { __schema: PartialIntrospectionSchema };\n\nexport const buildClientSchema = ({\n  __schema,\n}: IntrospectionData): SchemaIntrospector => {\n  const typemap: Map<string, SchemaObject | SchemaUnion> = new Map();\n\n  const buildNameMap = <T extends { name: string }>(\n    arr: ReadonlyArray<T>\n  ): (() => { [name: string]: T }) => {\n    let map: Record<string, T> | void;\n    return () => {\n      if (!map) {\n        map = {};\n        for (let i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];\n      }\n      return map;\n    };\n  };\n\n  const buildType = (\n    type: IntrospectionType\n  ): SchemaObject | SchemaUnion | void => {\n    switch (type.kind) {\n      case 'OBJECT':\n      case 'INTERFACE':\n        return {\n          name: type.name,\n          kind: type.kind as 'OBJECT' | 'INTERFACE',\n          interfaces: buildNameMap(type.interfaces || []),\n          fields: buildNameMap(\n            type.fields.map(field => ({\n              name: field.name,\n              type: field.type,\n              args: buildNameMap(field.args),\n            }))\n          ),\n        } as SchemaObject;\n      case 'UNION':\n        return {\n          name: type.name,\n          kind: type.kind as 'UNION',\n          types: buildNameMap(type.possibleTypes || []),\n        } as SchemaUnion;\n    }\n  };\n\n  const schema: SchemaIntrospector = {\n    query: __schema.queryType ? __schema.queryType.name : null,\n    mutation: __schema.mutationType ? __schema.mutationType.name : null,\n    subscription: __schema.subscriptionType\n      ? __schema.subscriptionType.name\n      : null,\n    types: undefined,\n    isSubType(abstract: string, possible: string) {\n      const abstractType = typemap.get(abstract);\n      const possibleType = typemap.get(possible);\n      if (!abstractType || !possibleType) {\n        return false;\n      } else if (abstractType.kind === 'UNION') {\n        return !!abstractType.types()[possible];\n      } else if (\n        abstractType.kind !== 'OBJECT' &&\n        possibleType.kind === 'OBJECT'\n      ) {\n        return !!possibleType.interfaces()[abstract];\n      } else {\n        return abstract === possible;\n      }\n    },\n  };\n\n  if (__schema.types) {\n    schema.types = typemap;\n    for (let i = 0; i < __schema.types.length; i++) {\n      const type = __schema.types[i];\n      if (type && type.name) {\n        const out = buildType(type);\n        if (out) typemap.set(type.name, out);\n      }\n    }\n  }\n\n  return schema;\n};\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport { getTypeCondition } from './node';\nimport { SchemaIntrospector, SchemaObject } from './schema';\n\nimport {\n  KeyingConfig,\n  UpdateResolver,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_NAME = '__';\n\nexport const isFieldNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  return !!field && field.type.kind !== 'NON_NULL';\n};\n\nexport const isListNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType =\n    field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;\n  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';\n};\n\nexport const isFieldAvailableOnType = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean =>\n  fieldName.indexOf(BUILTIN_NAME) === 0 ||\n  typename.indexOf(BUILTIN_NAME) === 0 ||\n  !!getField(schema, typename, fieldName);\n\nexport const isInterfaceOfType = (\n  schema: SchemaIntrospector,\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: string | void\n): boolean => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) {\n    return true;\n  } else if (\n    schema.types!.has(typeCondition) &&\n    schema.types!.get(typeCondition)!.kind === 'OBJECT'\n  ) {\n    return typeCondition === typename;\n  }\n\n  expectAbstractType(schema, typeCondition!);\n  expectObjectType(schema, typename!);\n  return schema.isSubType(typeCondition, typename);\n};\n\nconst getField = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n) => {\n  if (\n    fieldName.indexOf(BUILTIN_NAME) === 0 ||\n    typename.indexOf(BUILTIN_NAME) === 0\n  )\n    return;\n\n  expectObjectType(schema, typename);\n  const object = schema.types!.get(typename) as SchemaObject;\n  const field = object.fields()[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types!.has(typename) &&\n      schema.types!.get(typename)!.kind === 'OBJECT',\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types!.has(typename) &&\n      (schema.types!.get(typename)!.kind === 'INTERFACE' ||\n        schema.types!.get(typename)!.kind === 'UNION'),\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: SchemaIntrospector,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    for (const key in keys) {\n      if (!schema.types!.has(key)) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    }\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: SchemaIntrospector,\n  updates: Record<string, Record<string, UpdateResolver | undefined>>\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const mutationFields = (schema.types!.get(\n      schema.mutation\n    ) as SchemaObject).fields();\n    const givenMutations = updates[schema.mutation] || {};\n    for (const fieldName in givenMutations) {\n      if (mutationFields[fieldName] === undefined) {\n        warn(\n          'Invalid mutation field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n          21\n        );\n      }\n    }\n  }\n\n  if (schema.subscription) {\n    const subscriptionFields = (schema.types!.get(\n      schema.subscription\n    ) as SchemaObject).fields();\n    const givenSubscription = updates[schema.subscription] || {};\n    for (const fieldName in givenSubscription) {\n      if (subscriptionFields[fieldName] === undefined) {\n        warn(\n          'Invalid subscription field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n          22\n        );\n      }\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nfunction warnAboutAbstractResolver(\n  name: string,\n  kind: 'UNION' | 'INTERFACE'\n): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` does not match to a concrete type in the schema, but the \\`resolvers\\` option is referencing it. Implement the resolver for the types that ${\n      kind === 'UNION' ? 'make up the union' : 'implement the interface'\n    } instead.`,\n    26\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: SchemaIntrospector,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      if (schema.query) {\n        const validQueries = (schema.types!.get(\n          schema.query\n        ) as SchemaObject).fields();\n        for (const resolverQuery in resolvers.Query) {\n          if (!validQueries[resolverQuery]) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (!schema.types!.has(key)) {\n        warnAboutResolver(key);\n      } else if (\n        schema.types!.get(key)!.kind === 'INTERFACE' ||\n        schema.types!.get(key)!.kind === 'UNION'\n      ) {\n        warnAboutAbstractResolver(\n          key,\n          schema.types!.get(key)!.kind as 'INTERFACE' | 'UNION'\n        );\n      } else {\n        const validTypeProperties = (schema.types!.get(\n          key\n        ) as SchemaObject).fields();\n        for (const resolverProperty in resolvers[key]) {\n          if (!validTypeProperties[resolverProperty]) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: SchemaIntrospector,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const validMutations = (schema.types!.get(\n      schema.mutation\n    ) as SchemaObject).fields();\n    for (const mutation in optimisticMutations) {\n      if (!validMutations[mutation]) {\n        warn(\n          `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n          24\n        );\n      }\n    }\n  }\n}\n","import { stringifyVariables } from '@urql/core';\nimport { FieldArgs, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: FieldArgs) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n  Data,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OperationMap<T> = Map<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OperationMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: KeyMap<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OperationMap<KeyMap<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and deferred/streamed */\n  deferredKeys: Set<number>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOwnership: null | WeakSet<Data> = null;\nlet currentDataMapping: null | WeakMap<Data, Data> = null;\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentOptimistic = false;\n\n/** Creates a new data object unless it's been created in this data run */\nexport const makeData = (data?: Data): Data => {\n  let newData: Data;\n  if (data) {\n    if (currentOwnership!.has(data)) return data;\n    newData = currentDataMapping!.get(data) || ({ ...data } as Data);\n    currentDataMapping!.set(data, newData);\n  } else {\n    newData = {} as Data;\n  }\n\n  currentOwnership!.add(newData);\n  return newData;\n};\n\nexport const isWriting = (): boolean => currentOperation === 'write';\n\nexport const ownsData = (data?: Data): boolean =>\n  !!data && currentOwnership!.has(data);\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey?: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOwnership = new WeakSet();\n  currentDataMapping = new WeakMap();\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = new Set();\n  currentOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (currentOperation === 'read') {\n    // We don't create new layers for read operations and instead simply\n    // apply the currently available layer, if any\n    currentOptimisticKey = layerKey;\n  } else if (isOptimistic || data.optimisticOrder.length > 1) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      if (\n        data.optimisticOrder.indexOf(layerKey) !== -1 &&\n        !data.commutativeKeys.has(layerKey)\n      ) {\n        data.optimisticOrder.splice(data.optimisticOrder.indexOf(layerKey), 1);\n      }\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutative chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    // We also do this when only one layer exists to avoid having to squash\n    // any layers at the end of writing this layer\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock.has(data.optimisticOrder[i]) &&\n      data.commutativeKeys.has(data.optimisticOrder[i]) &&\n      !data.deferredKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOwnership = null;\n  currentDataMapping = null;\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    setTimeout(() => {\n      initDataState('read', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  if (layerKey && !isOptimistic) data.deferredKeys.delete(layerKey);\n  initDataState('write', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: new Map(),\n  refLock: new Map(),\n  links: {\n    optimistic: new Map(),\n    base: new Map(),\n  },\n  records: {\n    optimistic: new Map(),\n    base: new Map(),\n  },\n  deferredKeys: new Set(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic.get(currentOptimisticKey)!\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n  // A read may be initialised to skip layers until its own, which is useful for\n  // reading back written data. It won't skip over optimistic layers however\n  let skip =\n    !currentOptimistic &&\n    currentOperation === 'read' &&\n    currentOptimisticKey &&\n    currentData!.commutativeKeys.has(currentOptimisticKey);\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic.get(layerKey);\n    // If we're reading starting from a specific layer, we skip until a match\n    skip = skip && layerKey !== currentOptimisticKey;\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!skip || !currentData!.commutativeKeys.has(layerKey)) &&\n      (!currentOptimistic ||\n        currentOperation === 'write' ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: undefined | Set<string>,\n  refCount: KeyMap<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count and adjust it by \"by\"\n  const count = refCount.get(entityKey) || 0;\n  const newCount = count + by;\n  refCount.set(entityKey, newCount);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: undefined | Set<string>,\n  refCount: KeyMap<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      if (Array.isArray(link[i])) {\n        updateRCForLink(gc, refCount, link[i], by);\n      } else if (link[i]) {\n        updateRCForEntity(gc, refCount, link[i] as string, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic.get(currentData!.optimisticOrder[i]);\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  const { gc: batch } = currentData!;\n  for (const entityKey of batch.keys()) {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount.get(entityKey) || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey of currentData!.refLock.keys()) {\n      const refCount = currentData!.refLock.get(layerKey);\n      if (refCount) {\n        const locks = refCount.get(entityKey) || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        refCount.delete(entityKey);\n      }\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    currentData!.refCount.delete(entityKey);\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  }\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: KeyMap<number> | undefined;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: undefined | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount = data.refLock.get(currentOptimisticKey);\n    if (!refCount)\n      data.refLock.set(currentOptimisticKey, (refCount = new Map()));\n    links = data.links.optimistic.get(currentOptimisticKey);\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (\n  data: InMemoryData,\n  layerKey: number,\n  hasNext?: boolean\n) => {\n  if (hasNext) {\n    data.deferredKeys.add(layerKey);\n  } else {\n    data.deferredKeys.delete(layerKey);\n  }\n\n  let index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    if (hasNext || !data.commutativeKeys.has(layerKey)) {\n      data.optimisticOrder.splice(index, 1);\n      // Protect optimistic layers from being turned into non-optimistic layers\n      // while preserving optimistic data\n      clearLayer(data, layerKey);\n    } else {\n      return;\n    }\n  }\n\n  // If the layer has future results then we'll move it past any layer that's\n  // still empty, so currently pending operations will take precedence over it\n  for (\n    index = 0;\n    hasNext &&\n    index < data.optimisticOrder.length &&\n    !data.deferredKeys.has(data.optimisticOrder[index]) &&\n    (!data.refLock.has(data.optimisticOrder[index]) ||\n      !data.commutativeKeys.has(data.optimisticOrder[index]));\n    index++\n  );\n\n  data.optimisticOrder.splice(index, 0, layerKey);\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock.has(layerKey)) {\n    data.refLock.set(layerKey, new Map());\n    data.links.optimistic.set(layerKey, new Map());\n    data.records.optimistic.set(layerKey, new Map());\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock.has(layerKey)) {\n    data.refLock.delete(layerKey);\n    data.records.optimistic.delete(layerKey);\n    data.links.optimistic.delete(layerKey);\n    data.deferredKeys.delete(layerKey);\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = new Set();\n\n  const links = currentData!.links.optimistic.get(layerKey);\n  if (links) {\n    for (const entry of links.entries()) {\n      const entityKey = entry[0];\n      const keyMap = entry[1];\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    }\n  }\n\n  const records = currentData!.records.optimistic.get(layerKey);\n  if (records) {\n    for (const entry of records.entries()) {\n      const entityKey = entry[0];\n      const keyMap = entry[1];\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    }\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentOptimistic = true;\n    currentOperation = 'read';\n    const entries: SerializedEntries = makeDict();\n    for (const key of currentData!.persist.keys()) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    }\n\n    currentOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('write', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        if (readLink(entityKey, fieldKey) === undefined)\n          writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        if (readRecord(entityKey, fieldKey) === undefined)\n          writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import { CombinedError } from '@urql/core';\nimport {\n  GraphQLError,\n  FieldNode,\n  InlineFragmentNode,\n  FragmentDefinitionNode,\n} from 'graphql';\n\nimport {\n  isDeferred,\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField, isWriting } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  DataField,\n  NullArray,\n  Link,\n  Entity,\n  Data,\n} from '../types';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  parent: Data;\n  fieldName: string;\n  error: GraphQLError | undefined;\n  partial: boolean;\n  optimistic: boolean;\n  __internal: {\n    path: Array<string | number>;\n    errorMap: { [path: string]: GraphQLError } | undefined;\n  };\n}\n\nexport const contextRef: { current: Context | null } = { current: null };\nexport const deferRef: { current: boolean } = { current: false };\n\n// Checks whether the current data field is a cache miss because of a GraphQLError\nexport const getFieldError = (ctx: Context): GraphQLError | undefined =>\n  ctx.__internal.path.length > 0 && ctx.__internal.errorMap\n    ? ctx.__internal.errorMap[ctx.__internal.path.join('.')]\n    : undefined;\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean,\n  error?: CombinedError | undefined\n): Context => {\n  const ctx: Context = {\n    store,\n    variables,\n    fragments,\n    parent: { __typename: typename },\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    error: undefined,\n    partial: false,\n    optimistic: !!optimistic,\n    __internal: {\n      path: [],\n      errorMap: undefined,\n    },\n  };\n\n  if (error && error.graphQLErrors) {\n    for (let i = 0; i < error.graphQLErrors.length; i++) {\n      const graphQLError = error.graphQLErrors[i];\n      if (graphQLError.path && graphQLError.path.length) {\n        if (!ctx.__internal.errorMap)\n          ctx.__internal.errorMap = Object.create(null);\n        ctx.__internal.errorMap![graphQLError.path.join('.')] = graphQLError;\n      }\n    }\n  }\n\n  return ctx;\n};\n\nexport const updateContext = (\n  ctx: Context,\n  data: Data,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  contextRef.current = ctx;\n  ctx.parent = data;\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n  ctx.error = getFieldError(ctx);\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return (\n    isWriting() ||\n    !getSelectionSet(node).some(node => {\n      if (!isFieldNode(node)) return false;\n      const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n      return !hasField(entityKey, fieldKey);\n    })\n  );\n};\n\ninterface SelectionIterator {\n  (): FieldNode | undefined;\n}\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n): SelectionIterator => {\n  let childDeferred = false;\n  let childIterator: SelectionIterator | void;\n  let index = 0;\n\n  return function next() {\n    if (!deferRef.current && childDeferred) deferRef.current = childDeferred;\n\n    if (childIterator) {\n      const node = childIterator();\n      if (node != null) {\n        return node;\n      }\n\n      childIterator = undefined;\n      childDeferred = false;\n      if (process.env.NODE_ENV !== 'production') {\n        popDebugNode();\n      }\n    }\n\n    while (index < select.length) {\n      const node = select[index++];\n      if (!shouldInclude(node, ctx.variables)) {\n        continue;\n      } else if (!isFieldNode(node)) {\n        // A fragment is either referred to by FragmentSpread or inline\n        const fragmentNode = !isInlineFragment(node)\n          ? ctx.fragments[getName(node)]\n          : node;\n\n        if (fragmentNode !== undefined) {\n          const isMatching = ctx.store.schema\n            ? isInterfaceOfType(ctx.store.schema, fragmentNode, typename)\n            : isFragmentHeuristicallyMatching(\n                fragmentNode,\n                typename,\n                entityKey,\n                ctx.variables\n              );\n          if (isMatching) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(typename, fragmentNode);\n            }\n\n            childDeferred = !!isDeferred(node, ctx.variables);\n            if (!deferRef.current && childDeferred)\n              deferRef.current = childDeferred;\n\n            return (childIterator = makeSelectionIterator(\n              typename,\n              entityKey,\n              getSelectionSet(fragmentNode)!,\n              ctx\n            ))();\n          }\n        }\n      } else {\n        return node;\n      }\n    }\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x == null ? null : (x as Data | NullArray<Data>);\n\nexport const ensureLink = (store: Store, ref: Link<Entity>): Link => {\n  if (ref == null) {\n    return ref;\n  } else if (Array.isArray(ref)) {\n    const link = new Array(ref.length);\n    for (let i = 0, l = link.length; i < l; i++)\n      link[i] = ensureLink(store, ref[i]);\n    return link;\n  }\n\n  const link = store.keyOfEntity(ref);\n  if (!link && ref && typeof ref === 'object') {\n    warn(\n      \"Can't generate a key for link(...) item.\" +\n        '\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        ref.__typename +\n        '`.',\n      12\n    );\n  }\n\n  return link;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n  OptimisticMutationResolver,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  if (process.env.NODE_ENV !== 'production') {\n    invariant(\n      getMainOperation(request.query).operation === 'mutation',\n      'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n        'This case is unsupported and should never occur.',\n      10\n    );\n  }\n\n  initDataState('write', store.data, key, true);\n  const result = startWrite(store, request, {} as Data, undefined, true);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  isOptimistic?: boolean\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const kind = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    kind,\n    kind,\n    !!isOptimistic,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(kind, operation);\n  }\n\n  writeSelection(ctx, kind, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables,\n  fragmentName?: string\n) => {\n  const fragments = getFragments(query);\n  let fragment: FragmentDefinitionNode;\n  if (fragmentName) {\n    fragment = fragments[fragmentName] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'writeFragment(...) was called with a fragment name that does not exist.\\n' +\n          'You provided ' +\n          fragmentName +\n          ' but could only find ' +\n          Object.keys(fragments).join(', ') +\n          '.',\n        11\n      );\n\n      return null;\n    }\n  } else {\n    const names = Object.keys(fragments);\n    fragment = fragments[names[0]] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'writeFragment(...) was called with an empty fragment.\\n' +\n          'You have to call it with at least one fragment in your GraphQL document.',\n        11\n      );\n\n      return null;\n    }\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey,\n    undefined\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    warn(\n      \"Couldn't find __typename when writing.\\n\" +\n        \"If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\",\n      14\n    );\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iterate = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[ctx.optimistic ? fieldName : fieldAlias];\n\n    // Development check of undefined fields\n    if (process.env.NODE_ENV !== 'production') {\n      if (\n        !isRoot &&\n        fieldValue === undefined &&\n        !deferRef.current &&\n        !ctx.optimistic\n      ) {\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.',\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename && fieldName !== '__typename') {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    if (\n      // Skip typename fields and assume they've already been written above\n      fieldName === '__typename' ||\n      // Fields marked as deferred that aren't defined must be skipped\n      (fieldValue === undefined && deferRef.current)\n    ) {\n      continue;\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    // Execute optimistic mutation functions on root fields, or execute recursive functions\n    // that have been returned on optimistic objects\n    let resolver: OptimisticMutationResolver | void;\n    if (ctx.optimistic && isRoot) {\n      resolver = ctx.store.optimisticMutations[fieldName];\n      if (!resolver) continue;\n    } else if (ctx.optimistic && typeof fieldValue === 'function') {\n      resolver = fieldValue as any;\n    }\n\n    // Execute the field-level resolver to retrieve its data\n    if (resolver) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, data, typename, typename, fieldKey, fieldName);\n      fieldValue = ensureData(resolver(fieldArgs || {}, ctx.store, ctx));\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        (fieldValue !== null || !getFieldError(ctx)\n          ? fieldValue\n          : undefined) as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        updateContext(\n          ctx,\n          data,\n          typename,\n          typename,\n          joinKeys(typename, fieldKey),\n          fieldName\n        );\n\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link | undefined => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      // Add the current index to the walked path before processing the link\n      ctx.__internal.path.push(i);\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, data[i], indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (data === null) {\n    return getFieldError(ctx) ? undefined : null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    !ctx.store.keys[data.__typename] &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import * as InMemoryData from '../store/data';\nimport { FieldArgs } from '../types';\nimport { keyOfField } from '../store';\n\ninterface PartialFieldInfo {\n  fieldKey: string;\n}\n\nexport const invalidateEntity = (\n  entityKey: string,\n  field?: string,\n  args?: FieldArgs\n) => {\n  const fields: PartialFieldInfo[] = field\n    ? [{ fieldKey: keyOfField(field, args) }]\n    : InMemoryData.inspectFields(entityKey);\n\n  for (let i = 0, l = fields.length; i < l; i++) {\n    const { fieldKey } = fields[i];\n    if (InMemoryData.readLink(entityKey, fieldKey) !== undefined) {\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n    } else {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    }\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { TypedDocumentNode, formatDocument, createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  FieldArgs,\n  Link,\n  Data,\n  QueryInput,\n  UpdateResolver,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  Entity,\n  CacheExchangeOpts,\n} from '../types';\n\nimport { invariant } from '../helpers/help';\nimport { contextRef, ensureLink } from '../operations/shared';\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\nimport {\n  SchemaIntrospector,\n  buildClientSchema,\n  expectValidKeyingConfig,\n  expectValidUpdatesConfig,\n  expectValidResolversConfig,\n  expectValidOptimisticMutationsConfig,\n} from '../ast';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store<\n  C extends Partial<CacheExchangeOpts> = Partial<CacheExchangeOpts>\n> implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: Record<string, Record<string, UpdateResolver | undefined>>;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: SchemaIntrospector;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: C) {\n    if (!opts) opts = {} as C;\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = buildClientSchema(opts.schema);\n      queryName = schema.query || queryName;\n      mutationName = schema.mutation || mutationName;\n      subscriptionName = schema.subscription || subscriptionName;\n      // Only add schema introspector if it has types info\n      if (schema.types) this.schema = schema;\n    }\n\n    this.updates = {\n      [mutationName]: (opts.updates && opts.updates.Mutation) || {},\n      [subscriptionName]: (opts.updates && opts.updates.Subscription) || {},\n    };\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n\n    if (this.schema && process.env.NODE_ENV !== 'production') {\n      expectValidKeyingConfig(this.schema, this.keys);\n      expectValidUpdatesConfig(this.schema, this.updates);\n      expectValidResolversConfig(this.schema, this.resolvers);\n      expectValidOptimisticMutationsConfig(\n        this.schema,\n        this.optimisticMutations\n      );\n    }\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Entity) {\n    // In resolvers and updaters we may have a specific parent\n    // object available that can be used to skip to a specific parent\n    // key directly without looking at its incomplete properties\n    if (contextRef.current && data === contextRef.current.parent)\n      return contextRef.current!.parentKey;\n\n    if (data == null || typeof data === 'string') return data || null;\n    if (!data.__typename) return null;\n    if (this.rootNames[data.__typename]) return data.__typename;\n\n    let key: string | null | void;\n    if (this.keys[data.__typename]) {\n      key = this.keys[data.__typename](data);\n    } else if (data.id != null) {\n      key = `${data.id}`;\n    } else if (data._id != null) {\n      key = `${data._id}`;\n    }\n\n    return key ? `${data.__typename}:${key}` : null;\n  }\n\n  resolve(entity: Entity, field: string, args?: FieldArgs): DataField {\n    const fieldKey = keyOfField(field, args);\n    const entityKey = this.keyOfEntity(entity);\n    if (!entityKey) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link || null;\n  }\n\n  resolveFieldByKey = this.resolve;\n\n  invalidate(entity: Entity, field?: string, args?: FieldArgs) {\n    const entityKey = this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Entity): FieldInfo[] {\n    const entityKey = this.keyOfEntity(entity);\n    return entityKey ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery<T = Data, V = Variables>(\n    input: QueryInput<T, V>,\n    updater: (data: T | null) => T | null\n  ): void {\n    const request = createRequest<T, V>(input.query, input.variables as any);\n    request.query = formatDocument(request.query);\n    const output = updater(this.readQuery(request));\n    if (output !== null) {\n      startWrite(this, request, output as any);\n    }\n  }\n\n  readQuery<T = Data, V = Variables>(input: QueryInput<T, V>): T | null {\n    const request = createRequest(input.query, input.variables!);\n    request.query = formatDocument(request.query);\n    return read(this, request).data as T | null;\n  }\n\n  readFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    entity: string | Data | T,\n    variables?: V,\n    fragmentName?: string\n  ): T | null {\n    return readFragment(\n      this,\n      formatDocument(fragment),\n      entity,\n      variables as any,\n      fragmentName\n    ) as T | null;\n  }\n\n  writeFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    data: T,\n    variables?: V,\n    fragmentName?: string\n  ): void {\n    writeFragment(\n      this,\n      formatDocument(fragment),\n      data,\n      variables as any,\n      fragmentName\n    );\n  }\n\n  link(\n    entity: Entity,\n    field: string,\n    args: FieldArgs,\n    link: Link<Entity>\n  ): void;\n\n  link(entity: Entity, field: string, link: Link<Entity>): void;\n\n  link(\n    entity: Entity,\n    field: string,\n    argsOrLink: FieldArgs | Link<Entity>,\n    maybeLink?: Link<Entity>\n  ): void {\n    const args = (maybeLink !== undefined ? argsOrLink : null) as FieldArgs;\n    const link = (maybeLink !== undefined\n      ? maybeLink\n      : argsOrLink) as Link<Entity>;\n    const entityKey = ensureLink(this, entity);\n    if (typeof entityKey === 'string') {\n      InMemoryData.writeLink(\n        entityKey,\n        keyOfField(field, args),\n        ensureLink(this, link)\n      );\n    }\n  }\n}\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n  makeData,\n  ownsData,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n  deferRef,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data | null | undefined,\n  error?: CombinedError | undefined,\n  key?: number\n): QueryResult => {\n  initDataState('read', store.data, key);\n  const result = read(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data | null | undefined,\n  error?: CombinedError | undefined\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey,\n    false,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  if (!input) input = makeData();\n  // NOTE: This may reuse \"previous result data\" as indicated by the\n  // `originalData` argument in readRoot(). This behaviour isn't used\n  // for readSelection() however, which always produces results from\n  // scratch\n  const data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, input)\n      : readSelection(ctx, rootKey, rootSelect, input);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: ctx.partial || !data,\n    data: data || null,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  input: Data\n): Data => {\n  const typename = ctx.store.rootNames[entityKey]\n    ? entityKey\n    : input.__typename;\n  if (typeof typename !== 'string') {\n    return input;\n  }\n\n  const iterate = makeSelectionIterator(entityKey, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasChanged = false;\n  const output = makeData(input);\n  while ((node = iterate())) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = input[fieldAlias];\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (node.selectionSet && fieldValue !== null) {\n      dataFieldValue = readRootField(\n        ctx,\n        getSelectionSet(node),\n        ensureData(fieldValue)\n      );\n    } else {\n      dataFieldValue = fieldValue;\n    }\n\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== fieldValue;\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue!;\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n\n  return hasChanged ? output : input;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: Link<Data>\n): Link<Data> => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    let hasChanged = false;\n    for (let i = 0, l = originalData.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the root field's value\n      newData[i] = readRootField(ctx, select, originalData[i]);\n      hasChanged = hasChanged || newData[i] !== originalData[i];\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return hasChanged ? newData : originalData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    return readSelection(ctx, entityKey, select, originalData) || null;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables,\n  fragmentName?: string\n): Data | null => {\n  const fragments = getFragments(query);\n\n  let fragment: FragmentDefinitionNode;\n  if (fragmentName) {\n    fragment = fragments[fragmentName] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'readFragment(...) was called with a fragment name that does not exist.\\n' +\n          'You provided ' +\n          fragmentName +\n          ' but could only find ' +\n          Object.keys(fragments).join(', ') +\n          '.',\n        6\n      );\n\n      return null;\n    }\n  } else {\n    const names = Object.keys(fragments);\n    fragment = fragments[names[0]] as FragmentDefinitionNode;\n    if (!fragment) {\n      warn(\n        'readFragment(...) was called with an empty fragment.\\n' +\n          'You have to call it with at least one fragment in your GraphQL document.',\n        6\n      );\n\n      return null;\n    }\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename)\n    entity.__typename = typename;\n  const entityKey = store.keyOfEntity(entity as Data);\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeData()) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  input: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  const iterate = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let hasFields = false;\n  let hasPartials = false;\n  let hasChanged = typename !== input.__typename;\n  let node: FieldNode | void;\n  const output = makeData(input);\n  while ((node = iterate()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    if (fieldName === '__typename') {\n      // We directly assign the typename as it's already available\n      dataFieldValue = typename;\n    } else if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, output, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        output[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        output,\n        fieldArgs || ({} as Variables),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          dataFieldValue,\n          ownsData(input)\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (!node.selectionSet) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        (output[fieldAlias] !== undefined\n          ? output[fieldAlias]\n          : input[fieldAlias]) as Data,\n        resultValue,\n        ownsData(input)\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          (output[fieldAlias] !== undefined\n            ? output[fieldAlias]\n            : input[fieldAlias]) as Data,\n          ownsData(input)\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (dataFieldValue === undefined && deferRef.current) {\n      // The field is undelivered and uncached, but is included in a deferred fragment\n      hasFields = true;\n    } else if (\n      dataFieldValue === undefined &&\n      ((store.schema && isFieldNullable(store.schema, typename, fieldName)) ||\n        !!getFieldError(ctx))\n    ) {\n      // The field is uncached or has errored, so it'll be set to null and skipped\n      hasPartials = true;\n      dataFieldValue = null;\n    } else if (dataFieldValue === undefined) {\n      // If the field isn't deferred or partial then we have to abort\n      ctx.__internal.path.pop();\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = hasFields || fieldName !== '__typename';\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n    // Check for any referential changes in the field's value\n    hasChanged = hasChanged || dataFieldValue !== input[fieldAlias];\n    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue;\n  }\n\n  ctx.partial = ctx.partial || hasPartials;\n  return isQuery && hasPartials && !hasFields\n    ? undefined\n    : hasChanged\n    ? output\n    : input;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField,\n  skipNull: boolean\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const data = new Array(result.length);\n    let hasChanged =\n      !Array.isArray(prevData) || result.length !== prevData.length;\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        prevData != null ? prevData[i] : undefined,\n        result[i],\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childResult === undefined && _isListNullable);\n        data[i] = childResult != null ? childResult : null;\n        hasChanged = hasChanged || data[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? data : prevData;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (skipNull && prevData === null) {\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || makeData()) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  skipNull: boolean\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable = store.schema\n      ? isListNullable(store.schema, typename, fieldName)\n      : false;\n    const newLink = new Array(link.length);\n    let hasChanged =\n      !Array.isArray(prevData) || newLink.length !== prevData.length;\n    for (let i = 0, l = link.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the link\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined,\n        skipNull\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        ctx.partial =\n          ctx.partial || (childLink === undefined && _isListNullable);\n        newLink[i] = childLink || null;\n        hasChanged = hasChanged || newLink[i] !== prevData![i];\n      }\n    }\n\n    return hasChanged ? newLink : (prevData as Data[]);\n  } else if (link === null || (prevData === null && skipNull)) {\n    return null;\n  }\n\n  return readSelection(ctx, link, select, (prevData || makeData()) as Data);\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  Operation,\n  RequestPolicy,\n  CacheOutcome,\n  makeOperation,\n} from '@urql/core';\n\n// Returns the given operation result with added cacheOutcome meta field\nexport const addCacheOutcome = (\n  operation: Operation,\n  outcome: CacheOutcome\n): Operation =>\n  makeOperation(operation.kind, operation, {\n    ...operation.context,\n    meta: {\n      ...operation.context.meta,\n      cacheOutcome: outcome,\n    },\n  });\n\n// Copy an operation and change the requestPolicy to skip the cache\nexport const toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => {\n  return makeOperation(operation.kind, operation, {\n    ...operation.context,\n    requestPolicy,\n  });\n};\n","import {\n  Exchange,\n  formatDocument,\n  makeOperation,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  fromArray,\n  mergeMap,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { addCacheOutcome, toRequestPolicy } from './helpers/operation';\nimport { filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\nimport { Data, Dependencies, CacheExchangeOpts } from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype ResultMap = Map<number, Data | null>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Map<string, Operations>;\n\nexport const cacheExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts?: C\n): Exchange => ({ forward, client, dispatchDebug }) => {\n  const store = new Store<C>(opts);\n\n  if (opts && opts.storage) {\n    opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const operations: OperationMap = new Map();\n  const results: ResultMap = new Map();\n  const blockedDependencies: Dependencies = new Set();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = new Map();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep of dependencies.values())\n      if (blockedDependencies.has(dep)) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: undefined | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep of dependencies.values()) {\n        const keys = deps.get(dep);\n        if (keys) for (const key of keys.values()) pendingOperations.add(key);\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    for (const key of pendingOperations.values()) {\n      if (key !== operation.key) {\n        const op = operations.get(key);\n        if (op) {\n          operations.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    }\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.kind === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.kind === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      operations.delete(operation.key);\n      results.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.kind === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (dependencies.size) {\n        // Update blocked optimistic dependencies\n        for (const dep of dependencies.values()) blockedDependencies.add(dep);\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return makeOperation(\n      operation.kind,\n      {\n        key: operation.key,\n        query: formatDocument(operation.query),\n        variables: operation.variables\n          ? filterVariables(\n              getMainOperation(operation.query),\n              operation.variables\n            )\n          : operation.variables,\n      },\n      operation.context\n    );\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep of dependencies.values()) {\n      let depOps = deps.get(dep);\n      if (!depOps) deps.set(dep, (depOps = new Set()));\n      depOps.add(op.key);\n      operations.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const result = query(store, operation, results.get(operation.key));\n    const cacheOutcome: CacheOutcome = result.data\n      ? !result.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    results.set(operation.key, result.data);\n    updateDependencies(operation, result.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: result.data,\n      dependencies: result.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.kind === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    }\n\n    reserveLayer(\n      store.data,\n      operation.key,\n      operation.kind === 'subscription' || result.hasNext\n    );\n\n    let queryDependencies: void | Dependencies;\n    let data: Data | null = result.data;\n    if (data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(store, operation, data, result.error, key)\n        .dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(\n        store,\n        operation,\n        operation.kind === 'query' ? results.get(operation.key) || data : data,\n        result.error,\n        key\n      );\n\n      data = queryResult.data;\n      if (operation.kind === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n        results.set(operation.key, result.data);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      sharedOps$,\n      filter(op => {\n        return (\n          op.kind === 'query' && op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      sharedOps$,\n      filter(op => {\n        return (\n          op.kind !== 'query' || op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          blockedDependencies.clear();\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import { pipe, merge, makeSubject, share, filter, tap } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  Exchange,\n  ExchangeIO,\n  CombinedError,\n  createRequest,\n  makeOperation,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport {\n  SerializedRequest,\n  OptimisticMutationConfig,\n  Variables,\n  CacheExchangeOpts,\n} from './types';\n\nimport { cacheExchange } from './cacheExchange';\nimport { toRequestPolicy } from './helpers/operation';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = <T extends OptimisticMutationConfig>(\n  config: T,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || {};\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = <C extends Partial<CacheExchangeOpts>>(\n  opts: C\n): Exchange => input => {\n  const { storage } = opts;\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const { forward: outerForward, client, dispatchDebug } = input;\n    const { source: reboundOps$, next } = makeSubject<Operation>();\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: Operation[] = [];\n\n    const updateMetadata = () => {\n      const requests: SerializedRequest[] = [];\n      for (let i = 0; i < failedQueue.length; i++) {\n        const operation = failedQueue[i];\n        if (operation.kind === 'mutation') {\n          requests.push({\n            query: print(operation.query),\n            variables: operation.variables,\n          });\n        }\n      }\n      storage.writeMetadata!(requests);\n    };\n\n    let isFlushingQueue = false;\n    const flushQueue = () => {\n      if (!isFlushingQueue) {\n        isFlushingQueue = true;\n\n        for (let i = 0; i < failedQueue.length; i++) {\n          const operation = failedQueue[i];\n          if (operation.kind === 'mutation') {\n            next(makeOperation('teardown', operation));\n          }\n        }\n\n        for (let i = 0; i < failedQueue.length; i++)\n          client.reexecuteOperation(failedQueue[i]);\n\n        failedQueue.length = 0;\n        isFlushingQueue = false;\n        updateMetadata();\n      }\n    };\n\n    const forward: ExchangeIO = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.kind === 'mutation' &&\n            isOfflineError(res.error) &&\n            isOptimisticMutation(optimisticMutations, res.operation)\n          ) {\n            failedQueue.push(\n              incomingMutations.get(res.operation.context._instance as []) ||\n                res.operation\n            );\n            updateMetadata();\n            return false;\n          }\n\n          if (res.operation.kind === 'mutation' && !res.error) {\n            incomingMutations.delete(res.operation.context._instance as []);\n          }\n\n          return true;\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations) {\n        for (let i = 0; i < mutations.length; i++) {\n          failedQueue.push(\n            client.createRequestOperation(\n              'mutation',\n              createRequest(mutations[i].query, mutations[i].variables)\n            )\n          );\n        }\n\n        flushQueue();\n      }\n    });\n\n    const cacheResults$ = cacheExchange({\n      ...opts,\n      storage: {\n        ...storage,\n        readData() {\n          return storage.readData().finally(flushQueue);\n        },\n      },\n    })({\n      client,\n      dispatchDebug,\n      forward,\n    });\n\n    const incomingMutations = new WeakMap<[], Operation>();\n    return ops$ => {\n      const sharedOps$ = pipe(\n        ops$,\n        tap(operation => {\n          if (operation.kind === 'mutation') {\n            incomingMutations.set(operation.context._instance as [], operation);\n          }\n        }),\n        share\n      );\n\n      const opsAndRebound$ = merge([reboundOps$, sharedOps$]);\n\n      return pipe(\n        cacheResults$(opsAndRebound$),\n        filter(res => {\n          if (res.operation.kind === 'query' && isOfflineError(res.error)) {\n            next(toRequestPolicy(res.operation, 'cache-only'));\n            failedQueue.push(res.operation);\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n  }\n\n  return cacheExchange(opts)(input);\n};\n"],"names":["getFragmentTypeName","node","typeCondition","name","value","emptySelectionSet","alias","isFieldNode","selectionSet","selections","args","valueFromASTUntyped","arg","vars","getName","input","l","i","variableDefinitions","length","variable","key","def","currentDebugStack","cache","kind","identifier","popDebugNode","pop","pushDebugNode","typename","operation","getDebugOutput","join","condition","errorMessage","message","code","process","env","NODE_ENV","helpUrl","error","getMainOperation","doc","has","console","definitions","getFragments","Kind","OPERATION_DEFINITION","fragments","shouldInclude","directive","directives","isDeferred","arguments","__schema","buildNameMap","getTypeCondition","field","type","getField","schema","fieldName","expectAbstractType","ofType","expectObjectType","isInterfaceOfType","BUILTIN_NAME","warn","isSubType","indexOf","types","expectValidKeyingConfig","invariant","get","resolverQuery","keyOfField","stringifyVariables","deserializeKeyInfo","parentKey","fieldKey","parenIndex","slice","currentOwnership","currentDataMapping","currentOperation","currentData","newData","dotIndex","replace","data","isWriting","ownsData","currentDependencies","initDataState","currentOptimistic","set","optimisticOrder","WeakMap","isOptimistic","createLayer","layerKey","commutativeKeys","reserveLayer","clearDataState","currentOptimisticKey","getCurrentDependencies","persistData","refLock","deferredKeys","squashLayer","getCurrentOperation","defer","make","queryRootKey","storage","entity","entityKey","undefined","getNode","map","skip","updateRCForLink","optimistic","isArray","base","count","refCount","gc","newCount","add","delete","extractNodeMapFields","link","updateRCForEntity","Array","by","seenFieldKeys","linkNode","batch","keys","readRecord","records","links","updateDependencies","updatePersist","hasField","joinKeys","persist","serializeKeys","index","hasNext","setNode","clearLayer","splice","Map","deleteLayer","inspectFields","previousDependencies","entries","fieldInfos","hydrateData","Set","makeDict","x","current","makeContext","writeData","clear","contextRef","deferRef","getFieldError","ctx","__internal","path","errorMap","store","variables","isFragmentHeuristicallyMatching","__typename","parentTypeName","partial","makeSelectionIterator","next","graphQLErrors","childIterator","parent","parentFieldKey","select","isMatching","childDeferred","ensureData","fragmentNode","isInlineFragment","_node","getSelectionSet","startWrite","request","ref","writeSelection","keyOfEntity","fragment","fragmentName","result","normalizeVariables","query","isRoot","isQuery","iterate","writeField","fieldArgs","fieldValue","fieldAlias","InMemoryData","updateContext","push","resolver","KEYLESS_TYPE_RE","writeRecord","updater","constructor","indexKey","Store","prototype","__init2","queryName","test","mutationName","childKey","opts","resolvers","rootNames","arr","buildType","fields","abstractType","mutation","mutationType","abstract","possible","possibleType","typemap","out","buildClientSchema","subscription","subscriptionName","this","updates","Subscription","resolveFieldByKey","givenMutations","mutationFields","subscriptionFields","givenSubscription","_fieldName","expectValidResolversConfig","warnAboutAbstractResolver","validTypeProperties","resolverProperty","warnAboutResolver","validMutations","fieldInfoOfKey","JSON","parse","id","createRequest","readFragment","writeFragment","argsOrLink","maybeLink","resolve","rootKey","output","formatDocument","rootFields","Object","dataToWrite","expected","readRoot","writeLink","ensureLink","dataFieldValue","readRootField","rootSelect","readSelection","hasChanged","originalData","_key","isFieldAvailableOnType","hasPartials","resultValue","prevData","isFieldNullable","skipNull","hasFields","_isListNullable","newLink","childLink","resolveLink","childResult","_data","makeData","isDataOrKey","addCacheOutcome","meta","makeOperation","dep","context","forward","requestedRefetch","readData","then","readLink","optimisticKeysToDependencies","mutationResultBuffer","prepareForwardedOperation","operations","collectPendingOperations","pendingOperations","dependencies","executePendingOperations","op","filterVariables","policy","client","reexecuteOperation","operationResultFromCache","outcome","cacheOutcome","values","depOps","deps","queryResult","results","extensions","dispatchDebug","queryDependencies","ops$","share","filter","requestPolicy","nonCacheOps$","sharedOps$","nonOptimisticResults$","optimisticMutationCompletion$","res","shift","fromArray","source","cacheOps$","merge","cacheMissOps$","result$","isOfflineError","networkError","response","navigator","onLine","offlineExchange","onOnline","readMetadata","writeMetadata","outerForward","updateMetadata","optimisticMutations","failedQueue","requests","mutations","flushQueue","_i","config","print","isOptimisticMutation","cacheExchange"],"mappings":";;;;;;;;AAgBA,IAAAA,sBAAAC,KAAAA,EAAAC,cAAAC,KAAAC;;AAOA,IAAAC,gBAAAA,KAAAJ,EAAAK,QAAAL,EAAAK,MAAAF,QAAAH,EAAAE,KAAAC;;;;AAaA,IAAAG,kBAAAN,KAAAA,EAAAO,eAAAP,EAAAO,aAAAC,aAAAJ;;;;;;;;;;EChBM,IAAAJ;IACE,SAAAS,IAAA;;MAED,IAAAN,IAAAO,EAAAC,EAAAR,OAAAS;MAEJ,IAAAT,WAAA;;;;QACDM,EAAAI,QAAAF,MAAAR;;;;EAQA,OAAAM;;;;EAKA,KAAAK,MAAAC,EAAAf;IACE;;;EAVJ,KAAA,IAAAgB,IAAA,GAAAD,IAAAf,EAAAiB,oBAAAC,QAAAF,IAAAD,GAAAC,KAAA;IAiBA,IAAAd,IAAAW,QAAAb,EAAAiB,oBAAAD,GAAAG;;;EAME,OAAAL;;;;;EAUG,KAAAA;IAAA,OAAAF;;;IAGH,KAAA,IAAAI,IAAA,OAAAhB,EAAAiB,oBAAAC,QAAAF,IAAAD,GAAAC,KAAA;UACEI,IAAAR,EAAAK;MACD,IAAAf,IAAAW,QAAAQ,EAAAF;;;;ECpEH,KAAA,IAAAC,KAAAN;;;;;EAyCAQ,OAAAA;;;;;AAME,IAAAC,IAAAC;;AACEC,IAAAA,IAAA;;AAIA,IAAAC,eAAA,MAAAJ,EAAAK;;IADKC,oBAGA5B;;;IAIPyB,IAAAI,IAAA,uBAAAA,OAAA;wBACEJ,EAAAA,sBAAA;IAdJA,IAAA,GAeGzB,EAAAE,OAAA,IAAAF,EAAAE,KAAAC,WAAA,aAfHH,EAAA8B;;IAkBAC,QAAA/B,EAAAE;;;;;;;AAcK,IAAA6B,iBAAA,MAAAT,EAAAJ,SAAA,mBAAAI,EAAAU,KAAA,QAAA,MAAA;;;OAGDC;IACA,IAAAC,IAAAC,KAAA,oBAAAC,IAAA;IAEJ,IAAA,iBAAAC,QAAAC,IAAAC;MAEAL,KAAAH;;IAGIR,QAAAY,IAAAA,MAAAD,IAAAM,IAAAJ;IACDK,EAAAvC,OAAA;IACH,MAAAuC;;;;AC1EA,SAAAC,KAAAA,GAAAC;EAGE,KAAApB,EAAAqB;IACEC,QAAAC,KAAAA,IAAAtB;IACED,MAAAY;;;;AAYN,IAAAO,mBAAAC;;IACAI,IAAAA,EAAAA,YAAA/B,GAAAQ,SAAAwB,EAAAC;;;;;;;;EASE,IAAAC,IAAA;EAGF,KAAA,IAAAlC,IAAA,GAAAA,IAAA2B,EAAAG,YAAA5B,QAAAF,KAAA;;;MAMEkC,EAAArC,QAAAb,MAAAA;;;;;;AAaG,IAAAmD,gBAAA,CAAAnD,GAAAY;;IAGH,IAAAwC,IAAApD,EAAAqD,WAAArC;IArBF,IAAAd,IAAAW,QAAAuC;;;aA+BI,cAAAlD,MAAAC,KAAAA;;;;;;AAWK,IAAAmD,aAAA,CAAAtD,GAAAY;EACF,KAAA,IAAAI,IAAA,GAAAhB,EAAAqD,cAAArC,IAAAhB,EAAAqD,WAAAnC,QAAAF,KAAA;;IAIJ,IAAA,oBAFGoC,IAEH;;QAED,QAAAA,EAAAG,UAAAvC;;UClDFwC,SAAAA,EAAAA,EAAAA,OAAAA;;;MAKEC,QAAAA;;;;;;ACAA,sBAAAC,OAAA1D;;EACA,SAAAC,oBAAA0D,EAAAC,KAAApC;;;;EAOC,IAAAmC,IAAAE,SAAAC,GAAAjC,GAAAkC;;;;EAEDC,QAAA/D,iBAAA6D,KAAAA,OAAAH,EAAAC,KAAAK,SAAAN,EAAAC;EACAM,OAAA,qBAAA,eAAAD,EAAAA,OAAAzC;;;0DAjBF2C,yBAAAA;;AA0BE,wBAAAC,CAAAA,GAAAA,GAAAvC;EAMAqC;YAAAJ;;;;IAXF;;IAeIO,OAAApE,MAAA4B;;GA8DF,4BAAAiC;;;EAlDAI,iBAAAJ,GAAAjC;EA3BF,OAAAiC,EAAAQ,UAAArE,GAAA4B;;;;EAwCA,IAAA,MAAAkC,EAAAQ,QA7DAJ,SA6DA,MAAAtC,EAAA0C,QA7DAJ;IA6DA;;;UAEAL,EAAAU,MAAAV,IAAAjC;;IAaA,KAAA4C;MAIEpC,wCAAA,0BAAAR,+HAAA;;;EAGMwC,OAAAA;;;AAOH,SAAAH,iBAAAJ,GAAAjC;EACF6C,UAAAZ,EAAAU,MAAA5B,IAAAf,MAAA,aAAAiC,EAAAU,MAAAG,IAAA9C,GAAAL,MAAA,iBAAAa,QAAAC,IAAAC,WAAA,oCAAAV,IAAA,yFAAA,IAAA;;;;EA+EK+C,iBAAAvC,YAAAuC,YAAAP,4BAAAnE,qFAAA;;;AC/LR,IAfA2E,aAAA,CAAAd,GAAAtD,MAAAA,IAAA,GAAAsD,KAAAe,EAAArE,QAAAsD;;AAoBA,IAAAgB,WAAAA,CAAAA,GAAA3D,MAAA,GAAA4D,KAAA5D;;AAEE,qBAAA6D;;MAEAC,KAAA;IAAAD,OAAAA;MAAAA;MAJFlB,WAAAkB,EAAAE,MAAA,GAAAD;;;;IC8BAE,OAAAA;MACAC;MACAC,WAAAA;MACAC,WAAA;;;;;AAOE,IAAAR,qBAAA3D;;EAGEoE,OAAAA;eAFFpE,EAAA+D,MAAA,GAAAM,GAAAC,QAAA,QAAA;IAGEL,UAFAjE,EAAA+D,MAAAM,IAAiCE;;;;;;AAQnC,IAAAP,IAAA;;AACF,IAZAC,IAAA;;AAcA,IAAAO,IAAA;;AAEA,IAAAC,IAAA;;AAGA,IAAAC,IAAA;;;;AACA,IAAAC,KAAA;;;;EAWEC,IAAAL,GAAA;;;;IACAH,IAAAlD,YAAA;SAAAqD;;IAECN,EAAAY,IAAAN,GAAAH;;;;;;;;;;;EAmBKG,IAAAO,IAAAA;EALGb,IAAA,IAAAc;;;;EASLR,MAAAS;;;;;IAOFC,IAAAC;SACK,IAAA,WAAAhB;;;IA7CT,KAAAc,MAAAT,EAAAY,gBAAA3D,IAAA0D;MAuDAE,aAAAb,GAAAW;;MACAG,KAAA,MAAAA,EAAAA,gBAAAlC,QAAA+B,OAAAX,EAAAY,gBAAA3D,IAAA0D;;;;;;;SAeI;IASDI,IAAA;;;;;;MAQC,iBAAArE,QAAAC,IAAAC;IAhCJoE;;EAoCE,IAAAtE,IAAAkD;MACEe,IAAAI;;EAEEX,IAAA;EAEAa,IAAAA,OAAAV,gBAAA3B,QAAA+B,MAAA,GAAA;IAGF;IA5CJ,SAAAtF,KAAA,KAAA2E,EAAAkB,QAAAjE,IAAA+C,EAAAO,gBAAAlF,OAAA2E,EAAAY,gBAAA3D,IAAA+C,EAAAO,gBAAAlF,QAAA2E,EAAAmB,aAAAlE,IAAA+C,EAAAO,gBAAAlF;MAgDA+F,YAAApB,EAAAO,gBAAAlF;;;;EAQEyF,IAAA;EAPFnB,IAAA;EAUA0B,IAAAA;EACEtC,IAAAY;EADF,IAAA,iBAAAjD,QAAAC,IAAAC;IAYAjB,EAAAJ,SAAA;;EAUE,IAAA,WAAAmB,yBAAAsD,EAAAsB,OAAA;IATFtB,EAAAsB,SAAA;IAYAC,YAAAC;MACEF,cADF,QAAAtB,GAAA;;;MAAAc;;;;;;wBAWEH,GAAAF;;;;EAXFL,cAAA,SAAAJ,GAAAW,GAAAF;;;;AAkBEgB,IAAAA,sBAAA;EAlBF1C,UAAA,SAAAY,GAAA,iBAAAjD,QAAAC,IAAAC,WAAA,sKAAA,IAAA;EAqBA,OAAA+C;;;;;;;;;;MA2DM+B,IAAArH,EAAAiF,IAAAqC;EA7BN,SAAAC,MAAAF;;;;;;;;;;AAmDE,IAAAG,UAAA,CAAAC,GAAAH,GAAArC;;EAMF,IAAAyC,KAAA1B,KAAA,WAAAV,KAAAoB,KAAAnB,EAAAgB,gBAAA3D,IAAA8D;;IACAiB,IAAAA,IAAApC,kBAAAvE;IAME,IAAA4G,IAAAH,aAAA9C,IAAA2B;IAAAoB,SAEOG,MAAAnB;;aAGD1G,EAAAiF;;;EAXR,YAAAsC,OAgBGvH,IAAAyH,EAAAK,KAAAnD,IAAA2C,MAhBHtH,EAAAiF,UAAAsC;;;;EA2BM,IAAAQ,IAAAC,EAAArD,UAAA;;;EAKC,IAAAsD;IACF,IAAAC,KAAA;MAAAD,EAAAE,IAAAb;WAAA,IAAAS,KAAA,KAAAG,IAAA;MAAAD,EAAAG,OAAAd;;;;;AAKL,IAAAe,kBAAAA,CAAAA,GAAAL,GAAAM;;IAOEC,kBAAAN,GAAAD;;IAGA,SAAAhH,IAAAD,OAAAuH,EAAApH;UACEsH,MAAAX,QAAAS;;;QAEEC,kBAAAN,GAAAD,GAAAM,EAAAtH,IAAAyH;;;;;;;;;MAUJ,KAAAC,EAAA9F,IAAAqC,IAAA;;UACAkD;;;;;;;;;IAWI,IAAAP,IAAAH,EAAAG,WAAAjD,IAAAY,EAAAW,gBAAAlF;;;;;;;AAUFuE,IAAAA,KAAAA;OAIA0C,IAAAU;EAEE,KAAA,IAAArB,KAAAsB,UAAA;IAIH,KAFIrD,EAAAyC,SAAArD,IAAA2C,MAAA,KAEJ,GAAA;MApCHsB,EAAAR,OAAAd;;;kBA0CM/B,EAAA+B,QAAAuB,QAAA;;MAGD,IAAAb,GAAA;aACFA,EAAArD,IAAA2C,MAAA,KAGH;UAAA;;QACEtB,EAAAA,OAAAsB;;;;;IAMFwB,EAAAC,QAAAjB,KAAA7C;cAIEA,MAAAA,KAAAN,IAAA2C;IAJF,IAAAqB,GAAA;MAQApD,EAAAyD,MAAAlB,KAAAM,OAAAd;eACArC,KAAAA;wBAIEqC,GAAArC,EAAA+C,UAAAW,EAAA1D,KAAA;;;;;;AAUAgE,yBAAA3B,CAAAA,GAAArC;EACAiE,IAAA5B;;MANFxB,EAAAqC,IAAAb;WAUA6B,eAAAlE;MAIAa,EAAAqC,IAAAiB,SAAA9B,GAAArC;;;;;IAQEiE,gBAPF,CAAA5B,GAAArC;;IASEM,EATF8D,QAAAlB,iDAAAmB,CAAAhC,GAAArC;;;;;;EAgBI,OAAA+C,QAAAzC,WACEmB,GAAAA;;;IAKFuB,YAAAX,GAAArC;EAtBJgE,mBAAA3B,GAAArC;;;;AA8BEgE,sBAAAhE,GAAA9E;EACA+I,mBAAA5B,GAAArC;;2BA/BF9E;;;;EAyCAqG,IAAAA,IAAAA;EAMIb,IAAAA;EAEAA,IAAAA;;;;;;;;IAUCqC,IAAMrC,EAAAqC;;IAENC,IAAAtC,EAAAsC;;;EAKH,IAAAsB,IAAAC,KAGED,EAAArD;;;EA5BJuD,QAAA9D,EAAAqD,OAAA1B,GAAArC,GAAAqD;;;;;;MA+CIkB;IACA7D,EAAAoD;;IARJpD,EAAAmB,aAAAsB,OAAA9B;;;;IAeIX,IAAA6D,QAAAlD,gBAAA1D,IAAA0D,IAAA;;MAIDoD,WAAA/D,GAAAW;;MAGH;;;EAGE,KAAAiD,IAAA,GAAAC,KAAAD,IAAA5D,EAAAO,gBAAAhF,WAAAyE,EAAAmB,aAAAlE,IAAA+C,EAAAO,gBAAAqD,SAAA5D,EAAAkB,QAAAjE,IAAA+C,EAAAO,gBAAAqD,QAAA5D,EAAAY,gBAAA3D,IAAA+C,EAAAO,gBAAAqD,MAAAA;EAEE5D,EAAAA,gBAAAgE,UAAArD,GAAAA;EACDX,EAAAY,gBAAA4B,IAAA7B;;;;;IAMHS,EAAAA,gBAAAT,QAAAA;;EAGER,KAAAH,EAAAkB,QAAAjE,IAAA0D,IAAA;IAEA0C,EAAAA,QAAAzD,IAAAA,GAAA,IAAAqC;;IACAjC,UAAAiC,WAAA3B,IAAAK,GAAA,IAAAsD;;;;;;IAMGjE,EAAAkB,QAAAuB,OAAA9B;IACFX,EAAAoD,QAAAnB,WAAAQ,OAAA9B;;IAEDyC,EAAAA,aAAAxD,OAAAwD;;;;;;;IAOGpD,EAAAO,gBAAAyD,OAAAJ,GAAA;IACF5D,EAAAY,gBAAA6B,OAAA9B;;EAGDuD,WAAAlE,GAAAJ;;;AAIF,IAAAuE,cAAAxC;MACEyC,IAAAjE;EAAAiD,IAAAA,IAAAA;EAAA,IAAAxD,MAAAyD,MAAApB,WAAAjD,IAAA2B;EAEA,IAAAoC;;;;;;;;;;MAcEsB;;;;;;;;;EAMGlE,IAAMgD;;;;;cAQTvD,GAAAA,SACAA,KACDA;EApBH,IAAA0E,IAAA;EAuBAC,IAAAA,IAAAvE,IAAAwE;;;;;;;;;IAWMnE;QACEsB;IAED,IAAM0C,IAAAI;IAGN,KAAA,IAAAhJ,KAAAmE,EAAA8D,QAAAR,QAAA;MACF,KAAAvB,WACFA,GAAArC,eAEDwB,mBAAArF;MACA,IAAAiJ,SAAA;;UC1nBFjJ,KAAA,IAAA0D,EAAAuF;aAAA,SAAA9C,OAAA8C,IAAAvB,WAAAxB,GAAArC;QAAA+E,EAAA5I,KAAA0D,EAAAuF;;QACAC,EAAAlJ,UAAAmG;;;IAQAgD,KAAA;IASEhF,EAAA6B,QAAAoD,UAAAR;MAAAX,QAAAoB;;;;AA2BC,IAAAC,IAAA;;;;AAGH,IAvCAC,IAAA;EAyCAL,UAAA;;;IAUEM,gBAAAC,OAAAC,WAAAC,KAAA7J,SAAA,KAAA2J,EAAAC,WAAAE,WAAAH,EAAAC,WAAAE,SAAAH,EAAAC,WAAAC,KAAA/I,KAAA,aAAAuF;;IAEAgD,cAAA,CAAAU,MAAA/H,GAAArB,GAAAyF,GAAAM,GAAAnF;EACAoI,IAAA9G,IAAA;IACA8G;IAdFK;;IAiBAC,QAAAA;MAMEC,YAAevJ;;IAEfwJ,gBAAAxJ;IAEAwC;;IAiBIN,WAAA;IACAtB,YAAA8E;IACA+D,UAAA;;IA7BNR,YAAA;;MAsCAS,eAAAA;;;EAUE,IAAA9I,KAAA+I,EAAAA;IACE,KAAAb,IAAA3J,IAAA,GAAAA,IAAAyB,EAAAgJ,cAAwCd,QAAA3J,KAAA;;MAGtC,IAAAhB,EAAA0L,UAAAX,KAAA7J,QAAA;;;;;;;;;;;IAQEQ,gBAAA,IAAAiE,GAAA9D,GAAAyF,GAAArC,GAAAlB;EACD2G,EAAAJ,UAAAO;EACFA,EAAAc,SAAAhG;;EAEDkF,cAAAvD;EACEuD,EAAAe,iBAAAC;;;;;;EAKE,KAAAhK;YAAA;;;;;;EAKEgJ,iBAAAxI,QAAAC,IAAAwJ,YAAAzH,kFAEE8G,4CAAAD,IAFF,6CAAAjL,qJAAA;;IAQA,KAAAK;MAAA,QAAA;;IACE,IAAA2E,eAAApE,QAAAb,sBAAAA,GAAAY;IACEgB,4EAAAA,IAAAC;;;;4BAIF,CAAAA,GAAAyF,GAAAuE,GACEhB;EAEF,IAAAkB,KAAAL;EAMD,IAAAA;EACF,IAAAnC,IAAA;EACF,OAAM,SAAAiC;IACL,KAAAb,aAAAoB;MAAApB,EAAAL,UAAAyB;;IAEH,IAAAL,GAAA;MAvDH,IAAA1L,IAAA0L;MA2DFM,IAAA,QAAAA;eAGAhM;;;;UAKIkB,iBAAAF,QAAAsB,IAAAgG;QACE5G;;;IAEH,OAAA6H,IAAAsC,EAAA3K,QAAA;;;;;QAWA,IAAA+K,KAAAC,iBAAAC,KAAAtB,EAAA3H,UAAArC,QAAAsL,MAAAA;;cAEDtB,EAAAI,MAAAnH,SAAAK,kBAAA0G,EAAAI,MAAAnH,QAAAmI,GAAApK,KAAAsJ,gCAAAc,GAAApK,GAAAyF,GAAAuD,EAAAK;YCnMF,IAAA,iBAAA7I,QAAAC,IAAAC;;;YASEwJ,MAAAd,WAAAtF,GAAAlD,EAAAA;YACAgE,OAAA6D,WAAAyB;cAAApB,EAAAL,UAAAyB;;oBACAL,IAAAH,sBAAA1J,GAAAyF,GAAA8E,gBAAAH,IAAApB;;;;QASE,OAAAsB;;;;;;AAWF,IAAAH,aAAA3B,KAAA,QAAAA,IAAA,OAAAA;;AAGF,IAAAgC,aAAA,CAAApB,GAAAqB;;;;;;MAWEzB,EAAAN,KAAAA,WAAAU,GAAAsB,EAAAvL;;IAWEY;;EAGF4K,IAAAlE,IAAA2C,EAAAwB,YAAAF;EAEA,IAAA,iBAAAlK,QAAAC,IAAAC;IACEb,KAAAA,UAAA,mBAAA6K;MACDlI,KAAA,6HAAAkI,EAAAnB,aAAA,MAAA;;;EAKH,OAAA9C;;;AAUIoE,IAAAA,QAAA,CAAAxJ,GAAAA,GAAAyJ,GAAAA,GAAAvL;;;;EAYE,OAAAwL;;;;;;;;;;EA2BH,IAAA/B,IAAAN,YAAAU,GAAA4B,mBAAA/K,GAAAwK,EAAApB,YAAAnI,aAAAuJ,EAAAQ,QAAAtL,GAAAA,KAAA4E,GAAA3D;EAED,IAAA,iBAAAJ,QAAAC,IAAAC;IACEX,cAAAC,GAAAA;;EAGF2K,qBAAAvB,gBAAAnJ,IAAAoB;;IAWAxB;;EAlEF,OAAAkL;;;AAqJI,IAAAJ,iBAAA,CAAA3B,MAAAgB,GAAAlG;;MAEEoH,KAAAC;EAFF,IAAAnL;EAlDF,KAAAA,GAAA;;;oBA0DEmL,KAAA1F;;;EAIC,IAAA2F,IAAA1B,sBAAA1J,GAAAyF,KAAAzF,GAAAgK,GAAAhB;;;IAIC,IAAAvD,IAAAzG;;IAEE,IAAAoE,IAAAiI,WAAArC,GAAAsC;;IAOD,IAAMC,IAAAzH,EAAAkF,EAAAjD,aAAA7D,IAAAsJ;IAEN,IAAA,iBAAAhL,QAAAC,IAAAC;WACIwK,UAAAxF,MAAAD,MAAAqD,EAAAL,YAAAO,EAAAjD,YAAA;yBAEL0F,QAAAA,IAAA/K,YAAA0C,KAAAmI,sCAAAnI,IAAA,yIAAA,oBAAA;QAOD;;QAED8H,uBAAAlC,EAAAI,MAAAnH,QAAAjC,GAAAkC;;;QAIE,iBAAAA;MAEEwJ;;IA/FN1C,EAAAC,WAAAC,KAAAyC,KAAAH;IA+GC,IAAAI,SAAA;;MAIHC;QAAA;;;MAEAR,IAAAE;;IAQI,OAAA;6BAAAvL,GAAAoD,GAAAlB;;;;;gBAUEqF,SAVF9B,GAAArC;;QAYE4F,UAAAvD,KAAAzF,GAAAoD,GAAAqD;;;;;MAMHqF,YAAArG,KAAAzF,GAAAoD,GAAA,SAAAmI,MAAAxC,cAAAC,KAAAuC,SAAA7F;;QAGDwF,GAAA;MAEAnB,IAAAA,IAAAA;MAsBC,IAAAgC,GAAA;;QAEDjI,EAAA5B,KAAAqJ;;;;sBC5XEzL;;;;;;;;;IAOC,KAAM,IAAAX,IAAA,GAAAD,IAAA4E,EAAAzE,QAAAF,IAAAD,GAAAC,KAAA;MAEN6J,EAAAC,WAAAC,KAAAyC,KAAAxM;;MC8BH6M,IAAAA,IAAAX,WAAArC,GAAAgB,GAAAlG,EAAA3E,IAAA8M;MAAAC,EAAAC,KAAAhF;MAAA+E,EAAAC,WAAAC,KAAAA;;;;;;;;;IAYIC,UAAArD,EAAAI,aAAAG,eAAA,SAAA9D,KAAA,mBAAAzF,MAAA6L,EAAAS,KAAAtM;MACAuM,0DAAAxC,iMAAA/J,uIAAAA,IAAA,+BAAA;;;EAID,IAAAwM,IAAA/G,KAAAsE;;;;;AA6BA,MAAAmC;;;;;;;IAKHtB,KAAAA,YAAA6B,EAAAC,aAAA;;;;;IAOE,IAAA5I,IAAAA;IAEA,IAAA2I,EAAAE,QAAAA;MAEA,QPzDyC,GAArChL;;YAHJC,eAAAgL;UAJF,IAAAhH;;YAaAiH,KAAAA,GAAA9K;;mBAKI,IAAA5C,IAAA,GAAAA,IAAAyN,EAAAvN,QAAAF;;;;YAKI2N,OAAAA;;;QAAA,IAAAD,YAAA9K;;;WAQJ,KAAA;;;;;;;gBAlBJA,MAAAD,EAAAC;;;;;;YAiCEY,OANF;;;;;;;QAYK,IAAAV,IAAM8K;;UAENC,UAAMrL,EAAAsL,eAAAtL,EAAAsL,aAEL5O,OAAA;;UAGDsE,YAAM+C;UAENjD,UAAAyK,GAAAC;;;;;mBAKH,IAAA,YAAAJ,EAAApN;;mBACAR,IAAAE,aAAA0N,EAAApN,QAAA,aAAAyN,EAAAzN;;;qBAEEuN,MAAAC;;;;;UAOJlL,UAAAoL;;YClHF9K,IAAAA,IAAAZ,EAAAgB,MAAAxD;;;cAEA,IAAAmO;gBAAAD,EAAAjJ,IAAArC,EAAA1D,MAAAiP;;;;;QAkBE,OAAAlL;QMkFEmL,CAAAd,EAAAxK;;;MAEE1C,IAAA0C,EAAAuL,gBAAAC;MAEAlO;QAAAmO,KAAAzL,SAAAA;;;IAGDyL,KAAAC,UAAA;;WAEDpE,EAAAA,WAAAkD,EAAAkB,QAAAC,gBAAA;;;aAGFpI;;;;;;;;;iBJ0IEpB,OAAAoB;MAfJJ,QAAA;;;;;MAqBEJ,SAAA,IAAA+C;aACEvC;QACKO,YAAA,IAAAgC;QACLvC,MAAA,IAAApC;;MAxBJ8D,SAAA;QA4BAnB,YAAA,IAAAgC;;;MACApC,cAAAC,IAAAH;MAKEf,iBALF,IAAA4D;;;;IAdIlE;IIhIF,IAAAyJ,KAAAA,UAAA,iBAAAA,QAAApN,cAAA;;QNWE,IAAA,yBAAAA,IAAAC;;;cAIA,KAAAwB,EAAAS,MAAAmL;gBAAAtL,yCAAAjD,IAAA,sFAAA;;;;;OMfFqD,CAAA8K,KAAAzL,QAAAyL,KAAA1G;;;UN4BE;;QAIA,IAAA/E,EAAAC,UAAA;UAAA,IAAA6L,IAAA9L,EAAAU,YAAAqK,UAAAF;UACE,IAAAgB,IAAAH;UAOC,KAAA,IAAAzL,KAAA4L;YARH,IAAA,iBAAAtN,QAAAC,IAAAC;cASC,SAAAgF,MAAAqI,EAAA7L;gBACFM,KAAA,8BAAAN,IAAA,wFAAA;;;;;;UAUH,IAAA8L,IAAA/L,EAAAU,UAAAV,EAAAuL,cAAAV;UAIE,IAAAmB,IAAAN,EAAA1L,EAAAO,iBAAAnE;;YAQF,IAAA,iBAAAmC,QAAAC,IAAAC;uBAIEgF,QAAAwI;;;;;;;OAiBK,SAAAC,2BAAAlM,GAAAyK;QACF,IAAM,iBAAAlM,QAAAC,IAAAC;;;QAOH0N,KAAAA,IAAA7O,KAAAmN;UAID,IAAM,YAAAnN;gBACL8O,EAAAA,OAAAA;;cAIE,SAAAA,KAAA3B,EAAA4B;gBACEC,KAAAA,EAAAA;kBACDA,kBAAA,WAAAxL;;;;cAIRwL,kBAAA;;iBAOD/N,OAAAE,MAAAK,IAAAxB;;iBAEC,IAAA,gBAAA0C,EAAAU,MAAAG,IAAAvD,GAAAI,QAAA,YAAAsC,EAAAU,MAAAG,IAAAvD,GAAAI;YApCQtB,OAAAsB,yBACF,iBAAAa,QAAAC,IAAAC,YAAA8B,KAAA,uBAAAnE,kJAAA,YAAAsB,IAAA,sBAAA,sCAAA;;YAsCL6O,IAAAH,MAAA1L,MAAAV,IAAA1C,GAAAuN;YAGA,KAAAE,IAAAsB,OAAA/O;cAAA,OAAA+O;gBACEC,kBAAAvB,IAAA,MAAAsB;;;;;QA3CK,IAAAjQ,GAAAsB;QM/ET+N,KAAAzL,QAAAyL,KAAAhB;OLxIF,SAAA1J,qCAAAd,GAAAA;QAGA,IAAA,yBAAAzB,IAAAC;UAGA+N;;QAEE,IAAApL,EAAAA,UAAA;;;YAII3B,IAAA,yBAAAgN,IAAAC;;gBAEGnM,KAAA,wCAAAwK,uGAAA;;;;;;;;;IK2IP/E,KAAAA,aAAAjF;;;;;;;;;;;;;;;IAaIwH,IAAAA;;;WAIJ,IAAA,QAAAvL,EAAA2P;MACEnE,IAAAA,GAAA3G,EAAA+K;;;;IAKFC,OAAAA,IAAA,GAAAhL,EAAA0B,cAAA6D,MAAAyB;;;IAeAiE,IAAAA,IAAA/L,WAAAlB,GAAAuH;IAME0F,IAAAtJ,IAAAiI;;;;;IAkBFjH,SAAAuI,MAAAzD;MAAA0D,OAAAA;;IAOE,OADAA,SAAAxJ,SACA;;EAIA2G;IACEX,yBAAAiC,KAAAwB;;;IA/LN,IAAAzJ,IAAAiI,KAAA9C,YAAApF;;;;QAuCMyF;YACA+B;;;;;;;;;MClBN/B,CAAAR,GAAA3G,GAAAlD;;EASEgE,cAAAY;IACA,IAAAC,IAAAiI,KAAA9C,YAAApF;IAVF,OAAAC,IAAAwC,cAAAxC,KAAA;;;QAqBEgF,IAAAoE,IAAA5O,OAAAhB,EAAAoK;IAEAL,EAAAN,QAAAA,EAAAsC,EAAAA;;IAWEjL,IAAAoP;MACD3E,WAAAkD,MAAAjD,GAAA2E;;;;;;;;EAaCvP,aAAAA,GAAA2F,GAAA6D,GAAAyB;IACD,OAAAgE,aAAApB,MAAA2B,EAAAxE,IAAArF,GAAA6D,GAAAyB;;;IH0FDrF,EAAAA,GAAAuD,GAAAI,GAAAkG,GAAAxE;MACA,IAAAI,IAAAC,aAAAF;MACA,IAAAjL;;cAEE6K,IAAAxJ,EAAAyJ,KAMKI;;UAEN,OAAA;;aAWC,YAFFqE,OAAAvI,KAAA3F,SAEE;;;;MAOA,IAAArB,IAAAS;MACE,IAAA+O,IAEEjE;oBAIAkE;WAKA3L;;MAUD,IAAA2B;MAEA,KAAAA;QACF,OAAA,iBAAAjF,QAAAC,IAAAC,WAAA8B,KAAA,sIAAAxC,IAAA,MAAA,WAAA;;MAICkC,IAAA,iBAAAA,YAAAxB;;;;;;;;MGjJF+I,CAAAiE,MAAA2B;;EAxCJ5I,KAAAjB,GAAA1D,GAAAkN,GAAAC;;IA6CAS,IAAAA,eAAAT,IAAAhQ,IAAA+P;;IASE,uBAAAvJ;MACEkK,UAAAlK,GAAAzC,WAAAlB,GAAAlD,IAAAgR,WAAAlC,MAAAjH;;;;;AAOF,YAAA,CAAA2C,MAAAtF,GAAAlD,GAAArB;;EACA,IAAApB,IAAAiN,KAAAA,GAAAX,GAAA3G,GAAAlD;EACEgE;EACA,OAAAmG;;;;;;EAKA,IAAA8E,IAAAA,gBAAA5P;;EAEE4P,IAAAC,iBAAAD;IAKD9P,cAAMoP,GAAAlP;;;;;EASP+I,IAAAA,IAAAmG,MAAArP,EAAAsJ,MAAAkG,WAAA,QAAAI,SAAA1G,GAAAmG,GAAAY,GAAA9Q,KAAA+Q,cAAAhH,GAAAmG,GAAAY,GAAA9Q;;;;EAMJ6Q,OAAAA;;;;;;;yBAQI7Q;EAIE0E;IACAsM,OAAAhR;;EAGD,IAAAmM,IAAA1B,sBAAAjE,GAAAA,GAAAuE,GAAAhB;;;EAGF;EAnBH,OAAA7K,IAAAiN,KAAA;;;;IA4BI,IAAA4E,SAAAhH;IAEA,IAAA0G,EAAAA,gBAAAnG,SAAA2G;MACDL,IAAAC,cAAA9G,GAAAuB,gBAAApM,IAAAgM,WAAAoB;;;;;IAaD,SAAA7F;MAAA0J,EAAA5D,KAAAqE;;;;EAaI,OAAAI,IAAAb,IAAAnQ;;;;;;;IAYA,KAAA,IAAAE,IAAA,GAAAD,IAAAgR,EAAA7Q,QAAAF,IAAAD,GAAAC,KAAA;MAEH6J,EAAAC,WAAAC,KAAAyC,KAAAxM;mCAICA;;;;IAYD,OAAA8Q,IAAAtM,IAAAuM;;IAED;;EAIA,IAAAlH,IAAAA,EAAAI,kBAAA8G;;IAcC,OAAAF,cAAAhH,GAAAvD,GAAAuE,GAAAkG,MAAA;;IAED,gBAAAlH,GAAAkH,EAAA3G,YAAAS,GAAAkG;;;;IAUApB,eAAA,CAAA1F,GAAA6B,GAAAzF,GAAA6D,GAAAyB;EAAA1B,IAAAA,IAAAA,aAAAA;EAAA,IAAAJ;EAGA,IAAAvD;IACA,iBAAA;MACE,iBAAAjD,QAAA/B,IAAAC,YAAA8B,KAAA,8FAKIwK,0BAEAsC,OAAAtI,KAPJ3F,GAAAlB;MAYD,OAAA;;;;MASM,iBAFPK,QAEOC,IAAAC,YAAA6I,KAAAA,kIAAA;MACL,OAAA/G;;;EAWF,IAAA4I,IAAA1B,oBAAAmB;;;;;EAKA,QAAA;IACA,yBAAApK,gBAAA+B,KAAA,gIAAAxC,IAAA,MAAA;;;EAGE,IAAA7B,yBAAAsC,IAAAC;;;;;;IAMAb;;EAGA,OAAAW;;;;;;;;;;;;;;;SAuBE,IAAAuK,KAAA/K,MAAAA,EAAAyF,YAAAlG;;;;EAMC,IAAA6L,IAAA1B,sBAAA1J,GAAAyF,GAAAuE,GAAAhB;;EAED6G,IAAAA,KAAA;;;;;;8BA0BE1R,GAAA6K;;;IAMD,IAAAmH,IAAA5I,SAAA9B,GAAArC;;;IAIF;;MAcMgN,uBAAAhH,EAAAnH,QAAAjC,GAAAkC;;;;;;;MAyBP2N,IAAAnK;;;;;;MAcEmK,IAAAnD,EAAAxK,GAAAkN,GAAA9D,KAAA,IAAAlC,GAAAJ;;;;;;;;MAcJmC,IAAAkF;WArMF,SAAA3K,MAAA4K;gCA4MAtH,GAAAhJ,GAAAgK,GAAAuG,4BAAA7K,MAAA0J,EAAA5D,KAAA4D,EAAA5D,KAAAvM,EAAAuM,IAAA8E,GAAAtM,SAAA/E;WAUE;MACEmK,IAAAA,IAAAA,SAAAA,GAAAA;;;aAGA,uBAAAmC;;;;;;WAqBE,SAfF7F,MAeEmK,MAfFzG,EAAAnH,UAAAuO,gBAAApH,EAAAnH,QAAAjC,GAAAkC,MAAA6G,cAAAC,KAAA;;MAiBE6G,IAAA;WACE,SAAAnK,MAAAA,GAAA;QAEAuD,WAAAC;;;;;iBAUCuH;;;;;;EAQLzH,EAAAS,UAAAjH,aAAAA;EAQA,OAAA2I,KAAAkF,MAAAK,SAAAhL,IAAAuK,IAAAb,IAAAnQ;;;;EAIJ,kBAAA8L,IAAA/K;SASEoJ,OACEA,KAAAA;IACA,IAAAuH,IAAAvH,EAAAnH;;IAIA,IAAAgO,KAAAtJ,MAAAX,QAAAuK,MAAAK,EAAAvR,aAAAA;IAEA,KAAA,IAAAF,IAAA,GAAAD,IAAAuH,EAAApH,QAAAF,IAAAA,QAAA;;MAIE,IAAA0R,IAAAC,sBAAA9H,MAAA9G,GAAA8H,SAAAuG,GAAA,GAAApR,SAAAuG,QAAA6K,IAAAE,EAJFtR,UAAAuG,GAAAqF,EAAA5L,IAAAsR;MAcEzH,EAAAC,WAAAC,KAAApJ;MAEA,eAAA+Q;QACE;aACK;QACL7H,EAAAS;QAEAmH,EAAAA,KAAA,QAAAG,IAAAA,IAAA;QACAd,IAAAA,KAAAW,EAAAzR,OAAAoR,EAAAA;;;;;IAMJ,OAAAxF;SACD,IAAA0F,KAAA,SAAAF;;SAEDP,IAAAA,YAAAhH,IAAAgB;IAhDF,IAAAgH,IAAAT,KAAAU;IAmDAC,OAAA1I,mBAAAA,IAAAwH,iBAEEjF,MAAAiG,KAAAzH,cAFFP,GAAAzJ,GAAAyK,GAAAgH,GAAAjG;;qBCnlBAoG,6BAAA,+CAAA,uGAAA;IAMIC;;;;;EAOJ,kBAAA3K,IAAA;IAIE4K,KAAAA,YAAArI;;ICcF,IAAA4H,IAAA,IAAAjK,MAAAF,EAAApH;IAAA,IAAA4Q,KAAAtJ,MAAAX,QAAAuK,MAAAK,EAAAvR,WAAAkR,EAAAlR;IAAA,KAAA,IAAAF,IAAA,GAAAD,IAAAuH,EAAApH,QAAAF,IAAAD,GAAAC,KAAA;;;MASG6J,EAAAC,WAAAC,KAAApJ;;QAID;aACA;QACAkJ,EAAAS,UAAAT,EAAAS,gBAAA/D,MAAAmL,KAAAF;QACAC,EAAAzR,KAAA0R,KAAA;QACAZ,SAAAW,EAAAzR,OAAAoR,EAAApR;;;IAII,OAAA8Q,IAAAW,IAAAU;SADF,IAAA,SAAA7K,KAAA,SAAA8J,KAAAE;;;;;;;;;;EAaIW,MAAA;OAAAnR,EAAUsR,QAAAH;;;;;gCAKhBC,EAAApR,EAAAN,MAAAM;OAAAsR;;;;2BAQMC;EAGE,IAAApI,IAAAqI,IAAAvF,MAAAO;;IAGCA,EAAAlH,QAAAmM,WAAAC,MAAAxJ;MNjBPnI,EAAAA,GAAAA,GAAAA;QAJFkE,cAAA,SAAAJ,GAAA;QAMEX,KAAAA,IAAAA,KANFgF,GAAA;UAOE4B,IAAAA,IAAAA,EAPFxK;UASEqB,SATF8E,MASE9E,GATF;YAUE6I,KAAAA,6BAGEvG,mBADF3D;YAAA,IAAA,QAAAjB,EAAA;cAZF,SAAAoH,MAAAkM,SAAAnM,GAAArC;gBAAAuM,UAAAlK,GAAArC,GAAAsL,KAAAC,MAAArQ,EAAAgF,MAAA;;mBAkBA,SAAAoC,MAAAuB,cAAA7D;cAAA0I,YAAArG,GAAArC,GAAAsL,KAAAC,MAAArQ;;;;QAIMsG;QAEAoE,EAAAA,UAAAC;;;;EMCH,IAAA4I,IAAA,IAAA9J;EACH,IAzDF+J,IAAA;;;EA4DE,IAAAC,IAAAA,IAAA9R;EACE,QAAA;;EAGC;;MAEC+R,IAAAA,MAAAzS;QAAA,QAAA;;;;;MASA0S,2BAAA,CAAAC,GAAAC;IAAAA,IAAAA;;;;;YAGED;;;;;;;IAOAD,gBAAAC;MACAE,iBAAAnS;QACD,IAAAoS,IAAAL,EAAAlP,IAAAvD;;YAGHgH,OAAAhH;kBAAA;wBAMM+S;YAKHrS,EAXHsG,OAAAhH;YA1FJgT,IAAA;;UA2GEC,EAAAC,mCAAAJ,GAAAE;;;;;EAOA,IAlHFR,4BAAA9R;;;WAsHEyS,IAAAzS,eAAAyS,EAAAA,MAAA;QAIEnM,OAAAtG,EAAAV;;;WAUEoT,mBAAAC;;;;;QLdA1O,uBAAAkF,EAAAtF,MAAAvE,IAAA;QACD,IAAAwL,IAAAP,WAAApB,GAAAqB,GAAA,SAAA/E,IAAA;QACFd;;;;QKoBD,KAAA,IAAA0M,KAAAa,EAAAU;;;;QAKEtT,IAAAA,IAAAA,IAAAA;QAAAU,yBAAAiS,GAAAC;;;;IAKEF,WAAAC,MAAAA;MACAL;MACD5G,OAAAoE,EAAApP,EAAAgL;;OAEDtG,EAAAyE;;EAQA,IAAAhC,qBAAA,CAAAiL,GAAAF;;;MAGE,KAAAW;QAAAC,EAAA3O,IAAAkN,WAAAhJ;;MAEA2J;MAEAD,EAAAgB,IAAAX,EAAApH,KAAAoH;;;EAYEJ,IAAAA,2BAAAC;;IAED,IAAAU,IAAA7H,EAAAjH,QAAAiH,EAAAtB,UAAA,QAAA,YAAA;IACFwJ,EAAM7O,IAAAnE,EAAAV,KAAAwL,EAAAjH;;IA5CT,OAAA;;;MAiDEA;MACEsD,cAAAA,EAAA+K;;;;6CAlDJe;;IA+DM,IAAA,eAAAb,EAAA1S,MAAA4R;;MAWAU,yBAAAC,GAAAC;;;IAiBAxN,aAAAyE,QAAAnJ,EAAAV,KAAA,mBAAA4T,EAAAA,QAAApI,EAAApD;IACE5F,IAAAA;IACAzB,IAAAA,IAAAyK,EAFFjH;IAAA,IAAAA,GAAA;;MAPAmO,4BAEEhS;;;sBAgBNA,EAAAN,MAKM;;;;;MAMEM,cAAAkR,EAAAA,MAAAA,EAAA5R;;QADF6T;;;;MAaEtP;;MAAAoP;MAOEzB;;;EAIJ,OAAA4B;IACEtR,IAAAA,IADFuR,EAAAD;cAAAzN,EAAA8M,yBAAA9M,CAAA2N,GAAAlB,KAIE,YAAAA,EAAA1S,QAAA,mBAAA0S,EAAAd,QAAAiC,eAJFD,CAKIjV;IALJ,IAAAmV,IAAAF,GAAAlB,KAAA,YAAAA,EAAA1S,QAAA,mBAAA0S,EAAAd,QAAAiC,eAAAD,CAAAG;IA7BA,IALNH,IAAA3N,GAAA+M;;;;;QA0DAgB,QAAA;;;OA1DA/N,sIAkEIwM;;sBAOJwB,GAAAjB,+BAMMtT;MACE,IAAA0L;QACD9K,WAAAkR,gBAAAlR,GAAA0S;;QAED/R,OAAAzB,EAAAyB;QACE+D,YAAAkP,EAAAX;;;;QAQF,iCAAAf;;eACA,IAAA2B,wBAAA7T,EAAAsR;;;;MAMAxG,iBAAAqH,QAAAA,IAAAA,cAAAF;QAEA6B,MAAAA;QA5BNzT;QAiCAL;;UAMJ3B,OAAAyM;;QCzWAiJ,QAAA;;;WAKE/T,gBAAAA,EAAA0S,WAAA,iBAAAkB,EAAA5T,UAAAsR,QAAAiC,gBAAAS;IAIA,QAAAX,EAAA9B,EAAA5L,EAAAmM,0BAAAnM,CAAAsO,EAAA,EAAAT,GAAAU;;MAEE,IAAAjC,IAAAnT,IAAAuJ;oCAEOyC,GAAAmH;MAILE;MAJK;QAMLmB,SAAA1B,EAAA9Q,IAAAgK,EAAA9K,UAAAV,MAAAgU,CAAAa;;UAIJtC,EAAAnG,KAAAZ;;;MAeA,KAAA,IAAA5L,IAAA,GAAAA,IAAA2S,EAAAzS,QAAAF;QAAAoG,aAAAA,EAAAA,MAAAA,EAAAA,GAAAA,UAAAA;;MACAA,EAAAqD;;;;;;;MAQEwJ,yBAAA3F,EAAAxM,WAAAiS;;;;;;;AAwBM,IAAAmC,iBAAAzT,KAAAA,OAAA0T,iBAAA1T,EAAA2T,aAAA,sBAAAC,cAAA,MAAAA,UAAAC,UAAA,kDAAAnI,KAAA1L,EAAA0T,aAAAhU;;AAEC,IAAAoU,kBAAAjI,KAAAxN;EACF,KAAAsG,cAEDkH;EAAA,IAAAlH,KAAAA,EAAAoP,YAAApP,EAAAqP,gBAAArP,EAAAsP,eAAA;kBAGAC,GAAAtC,0BAEAuC,KACD9V;0BAGHuS;IAIM,IAAAwD,IAAAvI,EAAA1G,cAEEsO;IAGAY,IAAAA,IAAAtJ;IAKA,IAAAoJ,iBAAA;MACD,IAAAG,IAAA;MAED,KAAArB,IAAA1U,IAAAc,GAAAA,IAAAN,EAAAN,QAAAF,KAAA;;;UAIA+V,EAAAvJ,KAAA;;YArBNtC,WAAApJ,EAAAoJ;;;;MA6BI9D,EAAAsP,cAAAM;;;IAUD,IAAAC,aAAA;;;;;;;;;QAWF,KAAA,IAAAC,IAAA,GAAAA,IAAAJ,EAAA5V,QAAAgW;;;;aAcG/B;QAHEyB;;;;;;;;;;;;;;;;;;;;;;QAYA,IAAA,eAAAlB,EAAA5T,UAAAN,uBAAAkU,EAAAjT,UAxGF,EAAA0U;;UAEIrK,IAAAA,IAAAsK,aAAAtV;;;UAIL,OAAA6B,IAAAnD,EAAAmB;;cACDyF;;;;;;;;;;UAQE,QAAA;UAyFEiQ,CAAAR,GAAAnB,EAAA5T,YAAA;;UAEEgV;UACA,QAAA;;QAGF,IAAA,eAAApB,YAAAlU,SAAAkU,EAAAjT;;;;UAMR6U,EAAApC;;;;;;;;;;;;;;;;;;;;;;;"}